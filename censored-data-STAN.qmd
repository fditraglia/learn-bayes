---
title: "Censored Data"
format: 
  html:
    embed-resources: true
---

## What is this?

This document provides some simple examples of working with **censored data** in STAN, building on [this article](https://mc-stan.org/docs/stan-users-guide/censored-data.html) from the STAN users guide. The aforementioned users guide draws a distinction between *censoring* and *truncation*, namely: 

> **Truncated data** are data for which measurements are only reported if they fall above a lower bound, below an upper bound, or between a lower and upper bound ... If the truncation points are unknown, they may be estimated as parameters.

> **Censoring** hides values from points that are too large, too small, or both. Unlike with truncated data, the number of data points that were censored is known. The textbook example is the household scale which does not report values above 300 pounds.

I'm not sure if these definitions are completely universal. Wooldridge (2010) distinguishes models with a "corner solution outcome" (Chapter 17) from "true data-censoring problems" (Chapter 19) as follows:

> The word "censored" implies that we are not observing the entire possible range of the response variable but that is not the case for corner solution reponses. For example in a model of charitable contributions, the variable we are interested in explaining ... is the actual amount of charitable contributions. That this outcome might be zero for a non-trival fraction of the population does not mean that chritable contributions are somehow "censored at zero." 

> Typically, data censoring arises because of a survey sampling scheme or institution constraints. There, we will be interested in an underlying response variable that we do not fully observe because it is censored above or below certain values.

So Wooldridge and the STAN users guide appear to agree on the definition of censoring, but choose to contrast it with different alternatives. Truncated data, as defined above, seems to be a simple special case of **sample selection**. For example, suppose we want to calculate the correlation between entrance exam scores and first-year university grades. If we only observe first-year university grades for students who were admitted, and we only admit students whose entrance exam score is above $L$, this could be viewed as a truncated data problem. 

## Bathroom Scale Example

This example comes from the [STAN users guide](https://mc-stan.org/docs/stan-users-guide/censored-data.html). Consider a bathroom scale with a maximum capacity of $U$ kilograms. If we weigh an object that is less than $U$ kg, the scale gives an accurate reading. Suppose there's no measurement error. If we weigh an object that is greater than or equal to the weight limit, the scale reads $\geq U$. Again assume there's no measurement error. In some datasets we might not know the censoring threshold, in which case we can treat it as a parameter to be estimated. But to keep things simple, suppose for the moment that $U$ is a known value.

Now suppose that we use the bathroom scale to weigh an iid sample of $n$ people drawn from a $\text{Normal}(\mu, \sigma^2)$ population. Our goal is to infer the parameters $\mu$ and $\sigma$. The full model is as follows:
$$
Y_i \sim\text{iid Normal}(\mu, \sigma^2), \quad C_i = \mathbb{1}(Y_i \geq U), \quad C_i \text{ observed}, \quad \text{observe } Y_i \text{ iff } C_i = 0.
$$
There are two different ways to approach this model:

1. Treat the $Y_i$ for which $C_i = 1$ as *parameters to be estimated*.  
2. Integrate out the observations of $Y_i$ for which $C_i = 1$ without estimating them.

Since the current version of STAN can only handle continuous parameter spaces, the first approach is only applicable to models with continuous censored data. The second approach is more general, and can be applied to discrete or continuous examples. We'll consider each approach in turn. But first we'll generate some simulated data for use below, and show that ignoring censoring gives misleading results. First we'll simulate the data:

```{r}
#| message: false
#| warning: false
library(tidyverse)
library(broom)
set.seed(1983)
U <- 150
N_total <- 500
mu_true <- 130
sigma_true <- 25
y_true <- rnorm(N_total, mu_true, sigma_true)
y_obs <- ifelse(y_true < U, y_true, NA)
```

Completely ignoring the censoring gives very misleading results in this example:
```{r}
y_obs |> 
  t.test(mu = mu_true) |> 
  tidy() |> 
  knitr::kable(digits = 2)
```

Filling in the value of $U$ for the censored observations works better, but given the true parameter values and the value of $U$ in this example, the results are still misleading:
```{r}
y_obs |> 
  replace_na(U) |> 
  t.test(mu = mu_true) |> 
  tidy() |> 
  knitr::kable(digits = 2)
```

The slightly tricky thing about censored data is that STAN, unlike R, doesn't operate on missing values, so we need to represent the censoring in a different way. We'll address this by creating a list that contains the following items: a vector with the non-censored observations, a scalar that denotes the number of non-censored observations, a scalar that denotes the number of censored observations, and a scalar that denotes the censoring threshold. (Ok you're technically right that R doesn't have scalars -- call them length one vectors if you must!) 
```{r}
dat <- list(y_obs = y_obs[!is.na(y_obs)], 
            N_obs = sum(!is.na(y_obs)), 
            N_cens = sum(is.na(y_obs)), 
            U = U)
```


### First Approach: Censored Data as Parameters 

We'll use `cmdstanr` to access STAN throughout this document. See `CmdStanR-getting-started.qmd` for more details. In the summary table, I'll only display inferences for `mu` and `sigma`, although inferences for each of the censored observations are also available. Here I use flat priors for `mu` and `sigma`, the default if you don't specify otherwise. This isn't a great choice in more complicated models, but it should work fine here. The STAN code for this model is as follows:
```{r}
#| warning: false
#| message: false
library(cmdstanr)

censored_normal_impute <- cmdstan_model('censored-normal-impute.stan')
censored_normal_impute$print()
```
The syntax here is relatively straightforward. Two things are worth commenting on. First, the `<lower=max(y_obs)>` in the `data { ... }` block isn't strictly necessary. This just provides a bit of sanity checking for the data: there should be no observed value of $Y$ that is above the censoring threshold. By adding this to the model we ensure that STAN will throw an error if we pass it data that violate the constraints of the model.

Second, this is essentially a random effects model: `y_cens` is the vector of unobserved random effects, all of which are drawn from a common normal distribution with mean `mu` and standard deviation `sigma`. This normal distribution serves as the likelihood for the uncensored observations and the "prior" for the censored observations, which are treated as parameters. But these parameters are essentially being "shrunk towards each other" because they come from a common distribution. 

We can fit and summarize the model as follows. I'll only display the inferences for $\mu$ and $\sigma$, since these are the parameters of interest. All of the diagnostics look good, the posterior inferences are accurate for both of parameters of interest:
```{r}
fit1 <- censored_normal_impute$sample(
  data = dat,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  refresh = 500
)

fit1$summary(variables = c('mu', 'sigma')) |> 
  knitr::kable(digits = 2)
```



### Second Approach: Integrate Out Censored Data 

In this approach, rather than treating them as parameters to estimate, we add terms to the likelihood function that directly account for the contribution made by the censored observations. This approach is a bit more general since it can be used for models with discrete outcome variables. My understanding is that it is also a bit more computationally efficient. 

A censored observation corresponds to the event $\{Y_i > U\}$. Under the model normal model from above, and conditional on the model parameters, this censoring event has the following probability:
$$
\mathbb{P}(Y_i > U|\mu, \sigma) = 1 - \mathbb{P}(Y_i \leq U|\mu, \sigma) = 1 - \Phi\left( \frac{U - \mu}{\sigma}\right)
$$
where $\Phi(\cdot)$ denotes the standard normal CDF. This gives the likelihood contribution from an observation with $C_i = 1$. In contrast, the likelihood of an observation that is *not* censored, $C_i = 0$, is given by
$$
f(Y_i|\mu, \sigma) = \varphi\left( \frac{Y_i - \mu}{\sigma}\right)
$$
where $\varphi(\cdot)$ denotes the standard normal pdf. Using these expressions, we can write the overall likelihood of *all the data*, both censored and uncensored, as follows:
$$
L(\mu, \sigma|\mathbf{y}, U) = \prod_{i=1}^n \varphi\left( \frac{y_i - \mu}{\sigma}\right)^{1 - C_i} \left[1 - \Phi\left( \frac{U - \mu}{\sigma}\right)\right]^{C_i}.
$$
Hence, the log-likelihood is given by
$$
\log L(\mu, \sigma|\mathbf{y}, U) = \sum_{i=1}^n (1 - C_i) \log \varphi\left( \frac{y_i - \mu}{\sigma}\right) + \sum_{i=1}^n C_i \log \left[1 - \Phi\left( \frac{U - \mu}{\sigma}\right)\right].
$$
Defining $N_\text{cens} = \sum_{i=1}^n C_i$, we can write this more simply as
$$
\log L(\mu, \sigma|\mathbf{y}, U) = \sum_{i=1}^n (1 - C_i) \log \varphi\left( \frac{y_i - \mu}{\sigma}\right) + N_\text{cens} \times \log\left[1 - \Phi\left( \frac{U - \mu}{\sigma}\right)\right]
$$
since the second sum only depends on the data through the number of censored observations.

At this point it is straightforward to implement the model in STAN by making a small change to the code from above. We handle the uncensored observations exactly as before. They key difference is that `y_cens` no longer appears in the `parameters { ... }` block, or indeed anywhere else in the code. Instead we use the `target +=` syntax, explained in the zero-inflated Poisson example from `poisson-regression-STAN.qmd`. To make a long story short, `target +=` effectively adds whatever is on the RHS of the `+=` to the overall log-likelihood of the model. Here we use this syntax to account for the likelihood contribution of the censored observations:
```{r}
censored_normal_integrate <- cmdstan_model('censored-normal-integrate.stan')
censored_normal_integrate$print()
```
The function [`normal_lccdf()`](https://mc-stan.org/docs/functions-reference/normal-distribution.html#stan-functions-22) is a numerically stable implementation of 
$$
f(x|\mu, \sigma) = \log \left[1 - \Phi\left( \frac{x - \mu}{\sigma}\right) \right]
$$
so `target += N_cens * normal_lccdf(u | mu, sigma);`  adds the second term from above to the log likelihood, namely 
$$
N_\text{cens} \times \log\left[ 1 - \Phi\left( \frac{u - \mu}{\sigma}\right)\right].
$$

Now we can fit and summarize the model. This time around the only two parameters are $\mu$ and $\sigma$, so there's no need to set `variables = c('mu', 'sigma')` to suppress the parameters we're not interested in. 
```{r}
mu_lower <- mean(y_obs, na.rm = TRUE)
sigma_lower <- sd(y_obs, na.rm = TRUE)

fit2 <- censored_normal_integrate$sample(
  data = dat,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  refresh = 500,
  init = \() list(mu = runif(1, mu_lower, 2 * mu_lower),
                  sigma = runif(1, sigma_lower, 2 * sigma_lower))
) 
fit2$summary() |> 
  knitr::kable(digits = 2)
```
There's just one wrinkle in the above code: I needed to define a custom initialization function for `sample()` to avoid the following error message:
```
Chain 1   Log probability evaluates to log(0), i.e. negative infinity.
Chain 1   Stan can't start sampling from this initial value.
.
.
.
Chain 1 Initialization between (-2, 2) failed after 100 attempts.
```
If I understand correctly, the problem here comes from normal_lccdf() being evaluated at values of mu and sigma that result in a tail probability that equals zero to numerical precision. STAN's default is to initialize in a range where these probabilities are vanishingly small for my simulation example. I posted about this on the [STAN message board](https://discourse.mc-stan.org/t/stan-users-guide-4-3-censored-data/32523) to ask for alternative solutions that might be a little less kludgy / more broadly applicable. I don't think this is a big impediment, but I also don't want to reinvent the wheel if other people have some good ideas! 

## Censored Poisson Example

We should triple-check this, but I believe the cell-suppression issue only concerns small but *non-zero* counts. In other words, my understanding is that we know for sure whether we have observed a zero. I've written this example accordingly.

Suppose that we have an iid collection of Poisson distributed counts. Any count above $\ell$ is observed and any count of zero is observed, but any count between $1$ and $\ell$ inclusive is unobserved:
$$
Y_i \sim \text{iid Poisson}(\mu), \quad C_i = \mathbb{1}(1 \leq Y_i \leq \ell), \quad C_i \text{ observed}, \quad \text{observe } Y_i \text{ iff } C_i = 0.
$$
Since the censored observations are discrete, we can't treat them as unknown parameters in STAN: for the time being the software can only sample continuous parameters. But we can integrate them out. Let $\texttt{dpois}(y|\mu)$ denote the Poisson pmf with mean $\mu$ evaluated at $y$ and $\texttt{ppois}(y|\mu)$ denote the corresponding CDF. Then the likelihood contribution of an uncensored observation is simply $\texttt{dpois}(Y_i|\mu)$, while that of a censored observation is given by
$$
\mathbb{P}(1 \leq Y_i \leq \ell) = \texttt{ppois}(\ell|\mu) - \texttt{dpois}(0|\mu).
$$
Thus, the likelihood of the complete data, both censored and uncensored, is given by
$$
L(\mu|\mathbf{y}, \ell) = \prod_{i=1}^n \texttt{dpois}(y_i|\mu)^{1 - C_i} \left[\texttt{ppois}(\ell|\mu) - \texttt{dpois}(0|\mu) \right]^{C_i}.
$$
Hence, the log-likelihood is given by
$$
\log L(\mu|\mathbf{y}, \ell) = \sum_{i=1}^n (1 - C_i) \log \left[ \texttt{dpois}(y_i|\mu)\right] + N_\text{cens} \log \left[\texttt{ppois}(\ell|\mu) - \texttt{dpois}(0|\mu)\right]
$$
defining $N_\text{cens} = \sum_{i=1}^n C_i$ as above. We can implement this in STAN as follows:
```{r}
censored_poisson <- cmdstan_model('censored-poisson.stan')
censored_poisson$print()
```
The code here is similar to that from the from the censored normal example. The functions `poisson_lcdf()` and `poisson_lpmf()` are the log of the Poisson CDF and pmf. The function [`log_diff_exp(real x, real y)`](https://mc-stan.org/docs/functions-reference/composed-functions.html#composed-functions), on the other hand, is an efficient and numerically stable implementation of $\log(e^x - e^y)$.

Now we can simulate some data and test it out. If we ignore the cell suppression, we obtain an overestimate of $\mu$:
```{r}
ell <- 3
N_total <- 500
mu_true <- 5
y_true <- rpois(N_total, mu_true)
y_obs <- ifelse((y_true == 0) | (y_true > ell), y_true, NA) 

y_obs |> 
  t.test(mu = mu_true) |> 
  tidy() |> 
  knitr::kable(digits = 2)
```
Now let's try using STAN. As above, I'll use an initialization function to address the starting value problem:
```{r}
dat <- list(y_obs = y_obs[!is.na(y_obs)],
            N_obs = sum(!is.na(y_obs)),
            N_cens = sum(is.na(y_obs)),
            ell = ell)


mu_upper <- mean(!is.na(y_obs))

fit3 <- censored_poisson$sample(
  data = dat,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  refresh = 500,
  init = \() list(mu = runif(1, 0, mu_upper))
)

fit3$summary() |> knitr::kable(digits = 2)
```
Here the posterior 90% credible interval covers the true parameter value, because we have correctly accounted for the cell suppression problem.


## Poisson Regression Example
This example is identical to the preceding one, except that we introduce a covariate $X_i$ that allows the Poisson rate to vary across observations. First I'll simulate some data from a Poisson regression model, and then I'll censor any values of `y` between `1` and `3`:

```{r}
set.seed(1848)
n <- 250
x <- runif(n, -1, 2)
ell <- 3
y_true <- rpois(n, exp(0.8 + 0.5 * scale(x)))
y_obs <- ifelse((y_true == 0) | (y_true > ell), y_true, NA)
```

The new ingredient in this model is an equation that relates the Poisson rate to $X_i$, namely
$$
\log(\mu_i) = \alpha + \beta (X_i - \bar{X})/S_X
$$
where I center and scale $X_i$ to make it easier to think about the meaning of the coefficients. The log-likelihood is almost identical to the one from above. The only change is that $\mu_i$ replaces $\mu$ throughout:
$$
\log L(\mu|\mathbf{y}, \ell) = \sum_{i=1}^n (1 - C_i) \log \left[ \texttt{dpois}(y_i|\mu_i)\right] + \sum_{i=1}^n C_i \log \left[\texttt{ppois}(\ell|\mu_i) - \texttt{dpois}(0|\mu_i)\right].
$$
The simplest way to implement these two sums in the present example is to store the $X_i$ observations for the censored observations separately: notice that $X_i$ is always observed even when $Y_i$ is not!

```{r}
dat <- list(y_obs = y_obs[!is.na(y_obs)],
            x_obs = x[!is.na(y_obs)], 
            x_cens = x[is.na(y_obs)],
            N_obs = sum(!is.na(y_obs)),
            N_cens = sum(is.na(y_obs)),
            ell = ell)

censored_poisson_regression_model <- cmdstan_model('censored-poisson-regression-basic.stan')

censored_poisson_regression_fit <- censored_poisson_regression_model$sample(
  data = dat, 
  seed = 1234, # random seed for MCMC
  chains = 4, 
  parallel_chains = 4, 
  refresh = 500 # print status update after every 500 iterations
)

censored_poisson_regression_fit$summary() |> 
  knitr::kable(digits = 2)
```
It works quite well! In contrast, here are the results of a Poisson regression that ignores the censoring:
```{r}
dat_ignore <- data.frame(y = dat$y_obs, x = dat$x_obs)
glm(y ~ x, dat_ignore, family = poisson()) |> 
  tidy() |> 
  knitr::kable(digits = 2)
```


## Further Reading / References

- <https://discourse.mc-stan.org/t/censored-data-with-varying-known-censoring-points/15563> shows how to handle multiple known censoring thresholds, and suggests that the "integrating out" approach is actually more efficient