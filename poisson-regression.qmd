---
title: "Poisson Regression Examples from Statistical Rethinking"
format: 
  html:
    embed-resources: true
---

## Oceanic Tool Complexity Dataset

```{r}
#| message: false
#| warning: false
library(tidyverse)
library(rethinking)
data("Kline")
dat <- as_tibble(Kline)
rm(Kline)
dat
```

The goal is to model the total number of tools in a society, `total_tools`, in terms of `population` and the extent of contact with other islands, `contact`. 

### First Model
The first model is as follows:
$$
\begin{align*}
T_i &\sim \text{Poisson}(\lambda_i)\\
\log \lambda_i &= \alpha_{\text{CID}[i]} + \beta_{\text{CID}[i]} \log P_i\\
\alpha_j &\sim \text{to be specified}\\
\beta_j &\sim \text{to be specified}
\end{align*}
$$
where $T_i$ is `total_tools`, $P_i$ is `population` and $\text{CID}[i]$ is a categorical variable that indicates the value of `contact` for society $i$.
In other words, this is a model in which $\alpha$ and $\beta$ vary with `contact`.
Equivalently, the model includes a full set of dummy variables that encode `contact` along with interactions between these dummies and the log of `population`.
Notice that `population` is modeled on the log scale.
This is discussed further below when an explicit structural model replaces the log-linear predictor used here.

Now let's consider priors for the $\alpha_j$ and $\beta_j$ parameters. 
To make these easier to think about, it helps to write the model in terms of the centered and standardized value of $\log P_i$ rather than the raw value:  
```{r}
dat <- dat |> 
  mutate(cid = if_else(contact == 'low', 1, 2),
         lpop = log(population), 
         lpopz = (lpop - mean(lpop)) / sd(lpop)) |> 
  select(total_tools, cid, lpopz)
```
A value of zero for `lpopz` indicates that a given society has a log `population` equal to the sample mean.
This helps us think about a reasonable prior for $\alpha_j$


### A Prior for $\alpha$
Ignore `cid` for the moment,  and consider a society with log population equal to the sample mean. 
If we place a normal prior on $\alpha$, the model becomes
$$
\begin{align*}
T_i &\sim \text{Poisson}(\lambda_i)\\
\log \lambda_i &\sim \alpha \\
\alpha &\sim \text{Normal}(\mu, \sigma).
\end{align*}
$$

So what would be reasonable choices for $\mu$ and $\sigma$?
The key point is that $\alpha$ controls the expected number of tools for a society.
(Again, we're ignoring `contact` and considering the average society in terms of log population.)
Under the model, the expected value of $T_i$ is $\lambda_i = e^\alpha$.
And since $\alpha$ is normally distributed, $\lambda_i$ is [lognormal](https://en.wikipedia.org/wiki/Log-normal_distribution) with median $e^\mu$ and mean $\exp(\mu + \sigma^2 / 2)$.
A "vague" prior might set $\mu = 0$ and $\sigma = 10$.
But this is **totally crazy** in the present example: the implied lognormal distribution for $\lambda_i$ would have median $e^0 = 1$ and mean $\exp(50)$.
Even if we know little or nothing about oceanic societies, we'd be pretty confident that they should have at least a *handful* of tools, and that they're unlikely to more than a hundred tools.
The book suggests setting $\mu = 3$ and $\sigma = 0.5$, so that $\lambda_i$ has an *a priori* median of $\exp(3) \approx 20$ and mean of $\exp(3 + 0.5^2/2) \approx 23$.
```{r}
tibble(x = seq(from = 0.5, to = 100, by = 0.01),
       prior_Crazy = dlnorm(x, 0, 10),
       prior_Reasonable = dlnorm(x, 3, 0.5)) |> 
  pivot_longer(starts_with('prior'), names_prefix = 'prior_', 
               names_to = 'prior', values_to = 'density') |> 
  ggplot(aes(x = x, y = density, col = prior)) +
  geom_line() +
  theme_bw() + 
  xlab('mean number of tools')
```
### A Prior for $\beta$

Again, ignore for a moment the variable `contact`.
Given our normalization, the coefficient $\beta$ gives the effect of a one standard deviation change in log population on $\log \lambda_i$.
Suppose we choose to put a normal prior on $\beta$. 
Again we might be tempted to use a vague prior, e.g. $\text{Normal}(\mu = 0, \sigma = 10)$.
But this implies totally **crazy** marginal effects *a priori*.
The easiest way to see this is by simulation.
We'll stick with the $\text{Normal}(\mu = 3, \sigma = 0.5)$ prior for $\alpha$ and try the "vague" prior for $\beta$:
```{r}
set.seed(10)
n <- 100
tibble(i = 1:n, 
       a = rnorm(n, 3, 0.5),
       b = rnorm(n, 0, 10)) |> 
  expand_grid(x = seq(-2, 2, length.out = 100)) |> 
  ggplot(aes(x = x, y = exp(a + b * x), group = i)) +
  geom_line() +
  coord_cartesian(ylim = c(0, 1000)) +
  ylab('total tools') + 
  xlab('z-score of log population') +
  theme_bw()
```
It's wildly implausible that a tenth of a standard deviation change in log population should take total tools from close to zero to over a thousand. 
A more reasonable prior for $\beta$ would be something like $\text{Normal}(\mu = 0, \sigma = 0.2)$, which gives a much more reasonable picture: note the different y-axis
```{r}
set.seed(10)
n <- 100
tibble(i = 1:n, 
       a = rnorm(n, 3, 0.5),
       b = rnorm(n, 0, 0.2)) |> 
  expand_grid(x = seq(-2, 2, length.out = 100)) |> 
  ggplot(aes(x = x, y = exp(a + b * x), group = i)) +
  geom_line() +
  ylab('total tools') + 
  xlab('z-score of log population') +
  theme_bw()
```


### Simulating from the Posterior
Based on the experiments from above, we'll use the following priors:
$$
\begin{align*}
\alpha_j &\sim \text{Normal}(3, 0.5)\\
\beta_j & \sim \text{Normal}(0, 0.2)
\end{align*}
$$
```{r}
#| warning: false
#| message: false
m11_10 <- ulam(
  alist(
    total_tools ~ dpois(lambda),
    log(lambda) <- a[cid] + b[cid] * lpopz,
    a[cid] ~ dnorm(3, 0.5),
    b[cid] ~ dnorm(0, 0.2)
  ), data = dat, chains = 4, cmdstan = TRUE # faster!
)

precis(m11_10, depth = 2)
```
We can also look at some diagnostics:
```{r}
traceplot(m11_10)
```
```{r}
trankplot(m11_10)
```
We see that the model seems to be mixing well.
Here's the underlying STAN code used to fit the model:
```{r}
stancode(m11_10)
```

### A Structural Model

This is fun. 
Rather than simply using a log link function and a linear predictor on the log scale we can actually try to **think about a production technology for tools**!
Here's a simple model in discrete time suggested in the book.
Suppose that the change in the average number of tools from one period to the next is given by
$$
\Delta T = \alpha P^\beta - \gamma T.
$$
The parameter $\gamma$ represents *depreciation*: from one period to the next, a fraction $\gamma$ of existing tools falls out of use and disappears.
The parameters $\alpha$ and $\beta$ govern the creation of *new tools*.
The rate at which new tools are developed depends on population size according to a Cobb-Douglas model.
We would expect diminishing returns to population: $\beta < 1$.
Notice that this model is on the scale of *population* rather than the logarithm of population.
Notice further that this is a model in which population is exogenous.
We could think of more complicated models, but this is a very small and limited dataset.
In equilibrium $\Delta T = 0$, i.e.\ $\alpha P^\beta = \gamma T$.
Solving for the equilibrium number of tools $T^*$ gives
$$
T^* = \frac{\alpha}{\gamma} P^\beta.
$$
This is a structural model.
To turn it into a *statistical* model, we need a likelihood i.e.\ a distribution for *noise* around $T^*$. 
Since `total_tools` is a count variable with no upper bound, the maximum entropy distribution is a Poisson. 
So we specify the model as follows
$$
\begin{align*}
T_i &\sim \text{Poisson}(\lambda_i) \\
\lambda_i &= \alpha_{\text{CID}[i]} P^{\beta_{\text{CID}[i]}}/\gamma.
\end{align*}
$$
This allows the production technology for new tools, characterized by $\alpha$ and $\beta$, to vary depending on the extent of contact that the society had with other societies. 
The book doesn't mention this, but it seems that $\gamma$ shouldn't be separately identifiable.
If I'm right about this, the posterior for this parameter should equal the prior.
We'll take a look below.
All of the parameters should be positive.
The book places lognormal priors on $\alpha_{\text{CID}[i]}$ and standard exponential priors on $\beta_{\text{CID}[i]}$ and $\gamma$.
```{r}
#| warning: false
#| message: false
data(Kline)
dat2 <- as_tibble(Kline) 
rm(Kline)
dat2 <- dat2 |> 
  mutate(cid = if_else(contact == 'low', 1, 2)) |> 
  select(population, cid, total_tools)

m11_11 <- ulam(
  alist(
    total_tools ~ dpois(lambda),
    lambda <- exp(a[cid]) * population^b[cid] / g,
    # Not sure why the book uses a normal and then exponentiates rather
    # than just using a lognormal. Maybe to avoid numerical issues?
    a[cid] ~ dnorm(1, 1),
    b[cid] ~ dexp(1),
    g ~ dexp(1)
  ), data = dat2, chains = 4, cmdstan = TRUE # faster!
)
```
I think those warnings are OK: occasionally there's a numerical issue when working with constrained parameters.
But as long as it happens only occasionally, it should be fine. 
The STAN code for this model is very easy to understand: 
```{r}
stancode(m11_11)
```

Let's take a look at the results:
```{r}
precis(m11_11, depth = 2)
```
The other diagnostics look good:
```{r}
traceplot(m11_11)
```

```{r}
trankplot(m11_11)
```
So what about $\gamma$? Is it identifiable here? Or does the posterior simply match the prior?
```{r}
posterior <- extract.samples(m11_11)
mean(posterior$g)
tibble(g = posterior$g) |> 
  ggplot(aes(x = g)) +
  geom_density() +
  stat_function(fun = dexp, col = 'red') # defaults to exponential(1)
```


**add some plots etc**



## Over-dispersion

**Before putting this example in, I need to go back to the first example and make the plots that I skipped and look at the influence of Hawaii to the fits.**


## Simulation Example with Offsets

We often think of the Poisson parameter $\lambda_i$ as a *rate* of events per unit time or per unit space.
If the observations $i$ are recorded over *different* lengths of time or distance, then we need to account for this in our model.
We call these varying lengths of time or distance **exposures**.
The example from the book takes $Y_i$ to be the number of manuscripts produced by monastery $i$.
If one monastery records the number of manuscripts in a *day* while another records the number of manuscripts in a *week* we will need to account for this difference in our model.

If there are $\lambda$ events per unit time, then in an interval of length $\tau$ there will be, on average, $\mu = \lambda \tau$ events.
If, as above, we have a linear model on the log scale for $\lambda_i$, then
$$
\log \mu_i = \log (\lambda_i \tau_i) = \log \tau_i + \log \lambda_i = \log \tau_i + \alpha + \beta x_i.
$$
In other words: to translate from a model for the *rate* $\lambda_i$ to a model for the *mean number of events* $\mu_i$, on the log scale we merely need to subtract $\log \tau_i$.
The idea here is that $\tau_i$ is known, as in the monastery example.
In effect, we add another predictor to our log-linear model, $\log \tau_i$, but we *know in advance* that its coefficient is one. 
This term is called an **offset**.

Here's a little simulation example. 
There are two monasteries: the first has a rate of $\lambda = 1.5$ manuscripts per day and records daily totals; the second has a rate of $\lambda = 0.5$ manuscripts per day but records weekly totals.
```{r}
n0 <- 30 # observe daily counts for 30 days 
y0 <- rpois(n0, lambda = 1.5) # rate of 1.5 manuscripts / day 

n1 <- 4 # observe weekly counts for 4 weeks
y1 <- rpois(n1, lambda = 7 * 0.5) # rate of 0.5 manuscripts / day times 7 days

manuscripts <- tibble(y = c(y0, y1),
                      monastery = c(rep(0, n0), rep(1, n1)), # monastery dummy
                      exposure = c(rep(1, n0), rep(7, n1)), # exposure in days
                      log_days = log(exposure))
```

For this example, the book uses a normal approximation to the posterior rather than running Hamiltonian MC:
```{r}
m11_12 <- quap(
  alist(
    y ~ dpois(lambda),
    log(lambda) <- log_days + a + b * monastery, 
    a ~ dnorm(0, 1), # not great prior choices, but it won't matter!
    b ~ dnorm(0, 1)
  ), data = manuscripts
)
```
Now we can compute posterior predictions of the *daily rate* for each of the two monasteries:
```{r}
posterior <- extract.samples(m11_12)
lambda0 <- with(posterior, exp(a)) 
lambda1 <- with(posterior, exp(a + b))
precis(tibble(lambda0, lambda1, diff = lambda1 - lambda0))
```
So we correctly infer the difference in rates of manuscript production across the two monasteries.

## Zero-inflated Counts

This is a continuation of the monastery example.
Sometimes the monks spend the whole day drinking and don't work on any manuscripts. 
On these days, they produce zero manuscripts.
On other days, they work on manuscripts and finish a Poisson distributed number of them where the Poisson rate is $\lambda$ manuscripts per day.
It's possible that the monks will finish zero manuscripts even on a day that they spend working rather than drinking.
Because there are *two* ways to obtain zero manuscripts in this model, it won't match a standard Poisson distribution. 
Suppose that the monks flip a coin each day to decide whether to drink or work. 
The probability that they drink is $p$ and the probability that they work is $(1 - p)$.
Then the probability that they produce zero manuscripts on a give day is
$$
\begin{align*}
\mathbb{P}(Y = 0|p, \lambda) &= \mathbb{P}(Y=0|\text{Drink},p, \lambda) \mathbb{P}(\text{Drink}|p, \lambda) + \mathbb{P}(Y=0|\text{Work},p,\lambda) \mathbb{P}(\text{Work}|p,\lambda)\\
&= 1 \times p + \frac{e^{-\lambda}\lambda^0}{0!}\times (1 - p)\\
&= p + (1 - p) \exp(-\lambda)
\end{align*}
$$
while the probability that they produce $y > 0$ manuscripts is 
$$
\begin{align*}
\mathbb{P}(Y = y|p, \lambda) &= \mathbb{P}(Y=y|\text{Drink},p, \lambda) \mathbb{P}(\text{Drink}|p, \lambda) + \mathbb{P}(Y = y|\text{Work}, p, \lambda) \mathbb{P}(\text{Work}|p, \lambda)\\
&=  0 \times p + \frac{e^{-\lambda}\lambda^y}{y!} \times (1 - p)\\
&= (1 - p) \frac{\lambda^y \exp(-\lambda)}{y!}.
\end{align*}
$$
The preceding two equations completely describe a probability distribution for $Y$ that allows excess zeros: the **zero-inflated Poisson**. 
Let's call it $\text{ZIPoisson}(p, \lambda)$. 
We can use this distribution to build a regression model in which $p$ and $\lambda$ depend on covariates:
$$
\begin{align*}
Y_i &\sim \text{ZIPoisson}(p_i, \lambda_i)\\
\text{logit}(p_i)  &= \alpha_p + \beta_p x_i\\
\log(\lambda_i)  &= \alpha_\lambda + \beta_\lambda x_i
\end{align*}
$$
where $\text{logit}(p_i) \equiv \log[p_i / (1 - p_i)]$.
We could easily include more covariates and there's no need for the covariates in each equation to be the same: this is merely an example of what is possible.

Now we'll simulate some data from this model.
For simplicity, there's no regressor here but it would be easy (and a good exercise!) to add one:
```{r}
p_true <- 0.2 # Monks drink on 20% of days
lambda_true <- 1 # When working, monasteries average 1 manuscript / day

n <- 365 # A year's worth of daily data on manuscript production

set.seed(365) # This is the seed used in the book

drink <- rbinom(n, 1, p_true) # indicator for whether the monks drink
y <- (1 - drink) * rpois(n, lambda_true) # manuscripts produce
```
Note that we will only "observe" `y`; `drink` is merely used in the simulation. 
(If we knew `drink` we could simply analyze the days when the monks were working as a plain vanilla Poisson regression!)
Now we can fit the model:
```{r}
#| warning: false
#| message: false
m12_3 <- ulam(
  alist(
    y ~ dzipois(p, lambda),
    logit(p) <- ap,
    log(lambda) <- al,
    ap ~ dnorm(-1.5, 1),
    al ~ dnorm(1, 0.5)
  ), data = list(y = y), chains = 4, cmdstan = TRUE # faster!
)
```
These are the priors from the book.
It's worth taking a look at them before we fit the model.
The prior $\alpha_p \sim \text{Normal}(\mu = -1.5, \sigma = 1)$ is on the log-odds scale.
To convert it to a probability, we need to pass it through the inverse of $\text{logit}$, namely the standard logistic CDF `plogis()`:
```{r}
n_sims <- 50000
tibble(ap = rnorm(n_sims, -1.5, 1), p = plogis(ap)) |> 
  ggplot(aes(x = p)) +
  geom_density() +
  geom_vline(xintercept = p_true) +
  theme_bw()
```
This is a prior that "gently nudges" the estimated probability of drinking below 0.5. 
It's not a very stark prior, but it encodes the idea that the monks probably don't spend *most* days drinking. 
Similarly, we can examine the prior $\alpha_\lambda \sim \text{Normal}(\mu = 1, \sigma = 0.5)$ for the daily rate of manuscript production conditional on *not spending the day drinking*
```{r}
tibble(al = rnorm(n_sims, 1, 0.5), lambda = exp(al)) |> 
  ggplot(aes(x = lambda)) +
  geom_density() +
  geom_vline(xintercept = lambda_true) +
  theme_bw()
```
This is a prior that is fairly certain that the monks produce fewer than 10 manuscripts per day, on average.
Now that we've looked at the priors, let's look at a summary of the posterior:
```{r}
precis(m12_3)
```
As we did with the prior, we can transform these parameters to make them more interpretable:
```{r}
posterior <- extract.samples(m12_3)
tibble(ap = posterior$ap, p = plogis(ap)) |> 
  ggplot(aes(x = p)) +
  geom_density() + 
  geom_vline(xintercept = p_true) +
  theme_bw()
```
Similarly,
```{r}
tibble(al = posterior$al, lambda = exp(al)) |> 
  ggplot(aes(x = lambda)) +
  geom_density() +
  geom_vline(xintercept = lambda_true) +
  theme_bw()
```
So we see that the model is working well: the posteriors do not look like the priors, and they're tightly centered around the true parameter values.
The diagnostics for the sampler also look good:
```{r}
traceplot(m12_3)
trankplot(m12_3)
```
The STAN code for this one is a bit more complicated. 
There are some details about it on page 379 of the book.
In short: `dzipois()` is a function from the `rethinking` package that is implemented in a particular way to avoid numerical problems.
We can see exactly how this works in the STAN code:
```{r}
stancode(m12_3)
```
































