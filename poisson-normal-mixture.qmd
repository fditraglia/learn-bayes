---
title: "poisson-normal-mixture"
format: 
  html:
    embed-resources: true
---

## What is this?
A simulation to see if I understand how to estimate a Poisson mixture model correctly:
$$
\begin{align*}
Y_i &\sim \text{Poisson}(\lambda_i \nu_i)\\
\log(\lambda_i) &= \alpha + \beta X_i\\
\log(\nu_i) &\sim \text{Normal}(0, \sigma)
\end{align*}
$$
If $\sigma > 0$ we have a normal-Poisson mixture; if $\sigma = 0$ we have a plain-vanilla Poisson regression.

## Generate simulation data

Here the regression of `y` on `x` is a plain-vanilla Poisson regression.
The regression of `ymix` on `x` is a Poisson-normal mixture with the same conditional mean relationship between `ymix` and `x`.
(The intercept has a different interpretation: see the derivation below.)

```{r}
#| warning: false
#| message: false
library(rethinking)
library(tidyverse)
library(patchwork)

set.seed(298710)
n <- 1000
a <- -2
b <- 1

sim_dat <- tibble(x = runif(n, -3, 3),
                  u = rnorm(n, 0, 2), 
                  y = rpois(n, exp(a + b * x)), 
                  ymix = rpois(n, exp(a + b * x + u)))

dat <- sim_dat |> 
  select(x, y, ymix)

plain <- dat |> 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_smooth() +
  ggtitle('Plain Vanilla')

mix <- dat |> 
  ggplot(aes(x = x, y = ymix)) +
  geom_point() +
  geom_smooth() +
  ggtitle('Mixture')

plain + mix
```

## Do these really have the same conditional expectation function?
Drop $i$ subscripts for simplicity.
We have:
$$
\begin{align*}
Y &\sim \text{Poisson}(\mu)\\
\mu &= \lambda \nu \\
\log(\lambda) &= \alpha + \beta X\\
\log(\nu) &= U \sim \text{Normal}(0, \sigma)
\end{align*}
$$
If we treat $\alpha$ and $\beta$ as fixed parameters, $\lambda$ is $X$-measurable and thus
$$
\mathbb{E}[Y|X] = \mathbb{E}_{U|X}\left[\mathbb{E}\left(Y|X,U\right) \right] = \mathbb{E}[\lambda \nu|X] = \lambda \mathbb{E}[\nu|X]. 
$$
(Alternatively, re-interpret the expectation as conditional on $\alpha$ and $\beta$ in addition to $X$.)
Therefore, as long as $\nu$ is mean-independent of $X$ and $\mathbb{E}[\nu] = 1$ we have $\mathbb{E}[\lambda]$.

In the simulation above, $U$ is standard normal.
If $U\sim \text{Normal}(0, \sigma)$ then $\nu = \exp(U)$ is $\text{Lognormal}(0, \sigma)$.
Recall that a Lognormal$(\mu, \sigma)$ RV has mean $\exp(\mu + \sigma^2/2)$.
Thus, in our simulation $\mathbb{E}(\nu) = \exp(2^2/2) = \exp(2) \approx 7.4$.
This does not equal one. 
But we can always normalize $\nu$ to have expectation equal to one as follows:
$$
\begin{align*}
\lambda \nu &= \mathbb{E}(\nu)\exp(\alpha + \beta X) \left[\nu / \mathbb{E}(\nu) \right]\\
&= \exp\left\{\log(\mathbb{E}[\nu]) \right\}\exp(\alpha + \beta X)\tilde{\nu}\\
&= \exp\{\alpha + \log(\mathbb{E}[\nu]) + \beta X\}  \tilde{\nu}\\
&= \exp(\tilde{\alpha} + \beta X) \tilde{\nu}
\end{align*}
$$
where $\tilde{\alpha} \equiv \alpha + \log(\mathbb{E}[\nu])$ and $\tilde{nu} \equiv \nu/\mathbb{E}(\nu)$.
For example, in the simulation $\tilde{\alpha} = \alpha + \sigma^2/2 = (\alpha + 2)$.
So in sufficiently large samples, we should obtain the *same* estimates for $\beta$ from a plain-vanilla Poisson regression and a normal-Poisson mixture model, but the intercepts will differ by $\sigma^2/2$.


## Frequentist Estimation

First the "correctly specified" Poisson regression of `y` on `x`:
```{r}
library(broom)
glm(y ~ x, data = dat, family = poisson) |> 
  tidy() |> 
  knitr::kable(digits = 2)
```
Now the "mis-specified" Poisson regression of `ymix` on `x`:
```{r}
library(broom)
glm(ymix ~ x, data = dat, family = poisson) |> 
  tidy() |> 
  knitr::kable(digits = 2)
```

The slope coefficients behave as expect and so does the intercept: it should be zero in the second model since $-2 + 2^2/2 = 0$!

## Bayesian Estimation
Let's try to estimate this using the `rethinking package`.
At first I thought this would require working out the likelihood function for the normal-Poisson mixture.
This impression was based on the fact that `rethinking` (along with STAN) *does* have a built-in likelihood function for the gamma-Poisson mixture, i.e. negative binomial regression.
But I think it's equivalent, and simpler, to think of this mixture as a *hierarchical model*.
From page 407 of *Statistical Rethinking*:
> In the previous chapter (page 369), the beta-binomaial and gamma-Poisson models were presented as ways for coping with OVER-DISPERSION of count data. Varying intercepts accomplish the same thing, allowing count outcomes to be over-dispersed. They accomplish this because when each observed count gets its own unique intercept, but these intercepts are pooled through a common distribution, the predictions expect over-dispersion just like a beta-binomial or gamma-Poisson model would. Multilevel models are also mixtures. Compared to a beta-binomal or gamma-Poisson model, a binomial or Poisson model with a varying intercept on every observed outcome will often be easier to estimate and easier to extend.

I could be wrong--I should post on the STAN message board again and check--but this would seem to suggest that it will be sufficient to treat $\alpha$ as a heterogeneous parameter $\alpha_i \sim \text{Normal}(\bar{\alpha}, \sigma)$ and fit a multilevel model. 
That's the approach I try here.
I haven't thought carfully about the priors yet: I just want to see if the thing will run and there's plenty of data so they shouldn't matter:

```{r}
d <- dat |> 
  rowid_to_column('id') |>  # Calling it i throws a STAN error: it uses i internally
  select(x, y = ymix, id)

pois_mix <- ulam(
  alist(
    y ~ dpois(lambda),
    log(lambda) <- a[id] + b * x,
    a[id] ~ dnorm(a_bar, sigma),
    a_bar ~ dnorm(0, 1.5),
    b ~ dnorm(0, 0.2),
    sigma ~ dexp(1)
  ), data = d, chains = 4, cmdstan = TRUE
)
```

Ok, it seems to have worked. 
Let's take a look at the outputs:
```{r}
precis(pois_mix)
```
The estimate for $\sigma$ looks good. 
The estimate for $\beta$ is a little low, but I didn't think carefully about the prior.
It's also pretty close to the frequentist estimate.
Notice that the inferences are much more reasonable: the Frequentist model has the wrong standard errors.
The estimate for $\bar{a}$ also seems to equal the true $\alpha$! 
I *think* this is what's supposed to happen so perhaps I can declare victory here.
The R-hat values are a little high, and the effective sample sizes are a little low, so this model could probably benefit from more carefully considering the priors and possibly using a non-centered parameterization.








