---
title: "endogenous-poisson-normal-mixture"
format: 
  html:
    embed-resources: true
---

## The Model 
$$
\begin{align}
Y_i & \sim \text{Poisson}(\lambda_i)\\
X_i & \sim \text{Poisson}(\mu_i) \\
\log(\lambda_i) &= \alpha + \beta X_i + U_i \\
\log(\mu_i) &= \gamma + \delta Z_i + V_i \\
\begin{bmatrix}
U_i \\ V_i
\end{bmatrix} &\sim \text{Normal}( \mathbf{0}, \Sigma)\\
\Sigma &\equiv 
\begin{bmatrix} \sigma_U^2 & \sigma_U \sigma_V \rho \\
\sigma_U \sigma_V \rho & \sigma_V^2\end{bmatrix}
\end{align}
$$
```{r}
#| warning: false
#| message: false
library(rethinking)
library(tidyverse)
library(patchwork)
library(mvtnorm)

set.seed(298710)
n <- 2500
rho <- 0.9
sV <- 0.5
sU <- 0.4
S <- matrix(c(sU^2, rho * sU * sV, 
              rho * sU * sV, sV^2), 2, 2, byrow = TRUE)

errors <- rmvnorm(n, sigma = S)

a <- 3
b <- 1
g <- 0
d <- 0.2


sim_dat <- tibble(u = errors[, 1],
                  v = errors[, 2],
                  z = runif(n, -0.3, 0.3),
                  x = rpois(n, exp(g + d * z + v)),
                  y = rpois(n, exp(a + b * (x - mean(x)) + u)))


dat <- sim_dat |> 
  select(x, y, z)

y_z <- dat |> 
  ggplot(aes(x = z, y = y)) +
  geom_point()

y_x <- dat |> 
  ggplot(aes(x = x, y = y)) +
  geom_point()

x_z <- dat |> 
  ggplot(aes(x = z, y = x)) +
  geom_point()

y_z + y_x + x_z
```
## How much endogeneity have we created?
```{r}
library(broom)
glm(y ~ x, family = poisson, data = dat) |> 
  tidy() |> 
  knitr::kable(digits = 2)
```
It seems hard to generate significant bias in the plain-vanilly Poisson regression slope without using a totally crazy DGP. We need to think about this some more.


## Mullahy Approach

Suppose that
$$
\mathbb{E}(Y|X,\eta) = \exp(\alpha + \beta X) \eta = \exp(\alpha + \beta X + U)
$$
where $U \equiv \log(\eta)$. Then we can always write 
$$
Y = \mathbb{E}(Y|X,\eta) + \epsilon = \exp(\alpha + \beta X) \eta + \epsilon
$$
where $\mathbb{E}(\epsilon|X,\eta) = 0$. Now assume that the instrument $Z$ satisfies
$$
\mathbb{E}(Y|X,\eta,Z) = \mathbb{E}(Y|X,\eta) \quad \text{and} \quad \mathbb{E}(\eta|Z) = \tau.
$$
Then it follows that 
$$
\begin{align*}
\mathbb{E}(\epsilon|X,Z,\eta) &= \mathbb{E}[Y - \exp(\alpha + \beta X) \eta | X,Z,\eta] = \mathbb{E}(Y|X,Z,\eta) - \exp(\alpha + \beta X) \eta \\
&= \mathbb{E}(Y|X,\eta) - \mathbb{E}(Y|X,\eta) = 0
\end{align*}
$$
by the exclusion restriction from above. Now define
$$
T(Y,X;\alpha, \beta) = Y / \exp(\alpha + \beta X).
$$
Then we have
$$
\begin{align}
\mathbb{E}[T(Y,X;\alpha, \beta) - 1 | Z] &= \mathbb{E}\left[\left.\frac{Y}{\exp(\alpha + \beta X)}  - 1 \right| Z\right] \\
&= \mathbb{E}\left[ \left.\frac{\exp(\alpha + \beta X)\eta + \epsilon}{\exp(\alpha + \beta X)} - 1\right| Z\right] \\
&= \mathbb{E}[\eta - 1|Z] + \mathbb{E}\left[\left. \frac{\epsilon}{\exp(\alpha + \beta X)} \right|Z\right]\\
&= (\tau - 1) + \mathbb{E}\left[ \frac{1}{\exp(\alpha + \beta X)} \mathbb{E}\left(\left. \epsilon \right| X, Z, \eta\right)\right]\\
&= (\tau - 1) + \mathbb{E}\left[ \frac{1}{\exp(\alpha + \beta X)} \times 0\right]\\
&= (\tau - 1)
\end{align}
$$
And since we have an intercept $\alpha$ we can normalize $\tau$ to one so the moment condition becomes zero. Therefore, the conditional moment equalities are
$$
\mathbb{E}\left[ \left.\frac{Y}{\exp(\alpha + \beta X)} - 1 \right| Z\right] = 0.
$$
To carry out estimation, we can convert this into a collection of unconditional moment equalities by multiplying the the moment function by any function of $Z$:
$$
\mathbb{E}\left[ \left\{\frac{Y}{\exp(\alpha + \beta X)} - 1 \right\} \varphi(Z)\right] = 0.
$$
An obvious choice here would be $\varphi(Z) = (1, Z)$, so the moment equations would be
$$
\begin{align*}
\mathbb{E}\left[ \left\{\frac{Y}{\exp(\alpha + \beta X)} - 1 \right\} \right] &= 0\\
\mathbb{E}\left[ \left\{\frac{Y}{\exp(\alpha + \beta X)} - 1 \right\} Z\right] &= 0.
\end{align*}
$$
This is a two-dimensional non-linear optimization problem. Solving the first moment equation for $\alpha$ gives
$$
\alpha = \log \mathbb{E}[Y / \exp(\beta X)].
$$
In other words, $\exp(\alpha) = \mathbb{E}[Y/\exp(\beta X)]$. This allows us to "concentrate out" $\alpha$ from the second equation to obtain a one-dimensional root finding problem. Since this works for any $\varphi(Z)$ function, I'll present the general case:
$$
\mathbb{E}\left[ \left\{ \frac{Y}{\mathbb{E}[Y/\exp(\beta X)] \exp(\beta X) } - 1 \right\} \varphi(Z)\right] = 0.
$$
We can simplify this as to become
$$
\frac{1}{\mathbb{E}[Y/\exp(\beta X)]}\mathbb{E}\left[ \left\{ \frac{Y}{\exp(\beta X) } - \mathbb{E}\left[\frac{Y}{\exp(\beta X)}\right] \right\} \varphi(Z)\right] = 0.
$$
But since the first factor in the product can never be zero, the moment equation simplifies further to
$$
\mathbb{E}\left[ \left\{ \frac{Y}{\exp(\beta X) } - \mathbb{E}\left[\frac{Y}{\exp(\beta X)}\right] \right\} Z\right] = 0.
$$
Notice that, by the definition of covariance, this is simply
$$
\text{Cov}\left(\frac{Y}{\exp(\beta X)}, \varphi(Z)\right) = 0.
$$
for any function of $Z$, in particular for $\varphi(Z) = Z$.

Let's test this out on the simulation data:
```{r}
f <- function(b) {
  with(dat, cov(y / exp(b * x), z))
}
f_vectorized <- Vectorize(f)
b_seq <- seq(0.75, 2, 0.01)
plot(b_seq, f_vectorized(b_seq), type = 'l')
abline(h = 0, lty = 2, lwd = 2, col = 'red')
uniroot(f, c(0, 1.2))
uniroot(f, c(1.2, 2))
```
Let's check the implied value of alpha
```{r}
b_mullahy_1 <- uniroot(f, c(0, 1.2))$root
with(dat, log(mean(y / exp(b_mullahy_1 * x)))) # implied alpha

b_mullahy_2 <- uniroot(f, c(1.2, 2))$root
with(dat, log(mean(y / exp(b_mullahy_2 * x)))) # implied alpha
```
Interesting: two roots, one of which is "correct" and another of which is wrong but not very far away! I wonder if this is one of those situations in which you can (possibly) show point identification using the full set of conditional moment restrictions but a *particular* choice of unconditional moment equations doesn't necessarily give you point identification even if the number of equations is greater than or equal to the number of unknowns.

Let's check whether an additional moment restriction would rule out the extraneous root by evaluating an "overidentifying restriction" at a different function of $Z$
```{r}
with(dat, cov(y / exp(b_mullahy_1 * x), z^2))
with(dat, cov(y / exp(b_mullahy_1 * x), z^3))

with(dat, cov(y / exp(b_mullahy_2 * x), z^2))
with(dat, cov(y / exp(b_mullahy_2 * x), z^3))
```


















