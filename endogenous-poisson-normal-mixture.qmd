---
title: "endogenous-poisson-normal-mixture"
format: 
  html:
    embed-resources: true
---

## The Model 
We decided to use a simpler model, in which there is a single normal error term that is shared between the models for $Y$ and $X$. Our original model was this:
$$
\begin{align}
Y_i & \sim \text{Poisson}(\lambda_i)\\
X_i & \sim \text{Poisson}(\mu_i) \\
\log(\lambda_i) &= \alpha + \beta X_i + U_i \\
\log(\mu_i) &= \gamma + \delta Z_i + V_i \\
\begin{bmatrix}
U_i \\ V_i
\end{bmatrix} &\sim \text{Normal}( \mathbf{0}, \Sigma)\\
\Sigma &\equiv 
\begin{bmatrix} \sigma_U^2 & \sigma_U \sigma_V \rho \\
\sigma_U \sigma_V \rho & \sigma_V^2\end{bmatrix}
\end{align}
$$
There were two things we didn't like about this model. First, it allowed two sources of over-dispersion in each outcome equation: a common and idiosyncratic component. We thought it might be hard to distinguish these in the data and we also weren't really sure that these were both necessary or gave any interesting dynamics. We decided to set the correlation between $U_i$ and $V_i$ to one, which is equivalent to removing the idiosyntratic component of the overdispersion. We also decided that having $X_i$ enter on the raw scale in the equation for $\log(\lambda_i)$ was pretty crazy, since it means the effect of $Z_i$ is like $e^{e^z}$. So we decided to replace $X_i$ with $\log(1 + X_i)$. 

**Potentially undesireable feature: this seems to create a link between the strength of the first stage and the extent of endogeneity. This may be too restrictive, so we might want to add an additional error term in the $X$ equation.**


With these two changes, the model becomes:
$$
\begin{align}
Y_i & \sim \text{Poisson}(\lambda_i)\\
X_i & \sim \text{Poisson}(\mu_i) \\
\log(\lambda_i) &= \alpha + \beta \log(1 + X_i) + \sigma U_i \\
\log(\mu_i) &= \gamma + \delta Z_i + \eta U_i \\
U_i &\sim \text{Normal}(0, 1)
\end{align}
$$
Now we'll simulate from the model:
```{r}
#| warning: false
#| message: false
library(tidyverse)
library(patchwork)
library(mvtnorm)

set.seed(2817)
n <- 2500

# Under our scaling, alpha controls the rate of Y when X equals zero
alpha <- 4 # approx 50 on the log scale
beta <-  0.5 # partial effect is beta * exp(everything)
sigma <- 1 # calibrate based on expected variation in the rate of Y when X is zero

# Simpler to express the first-stage if we first center and scale z (z-score!)
gamma <- 0.5 # rate of X on log scale at average Z
delta <- 1
eta <- 0.8
  
sim_dat <- tibble(u = rnorm(n),
                  z = runif(n, -0.3, 0.3),
                  x = rpois(n, exp(gamma + delta * scale(z) + eta * u)),
                  y = rpois(n, exp(alpha + beta * log(1 + x) + sigma * u)))



dat <- sim_dat |> 
  select(x, y, z)

y_z <- dat |> 
  ggplot(aes(x = z, y = y)) +
  geom_point()

y_x <- dat |> 
  ggplot(aes(x = x, y = y)) +
  geom_point()

x_z <- dat |> 
  ggplot(aes(x = z, y = x)) +
  geom_point()

y_z + y_x + x_z
```
## How much endogeneity have we created?
```{r}
library(broom)
glm(y ~ log(1 + x), family = poisson, data = dat) |> 
  tidy() |> 
  knitr::kable(digits = 2)
```


## Mullahy Approach
**Note that $U$ and $\eta$ are defined in a different way in this section! I should come back and fix it later!**

Suppose that
$$
\mathbb{E}(Y|X,\eta) = \exp(\alpha + \beta X) \eta = \exp(\alpha + \beta X + U)
$$
where $U \equiv \log(\eta)$. Then we can always write 
$$
Y = \mathbb{E}(Y|X,\eta) + \epsilon = \exp(\alpha + \beta X) \eta + \epsilon
$$
where $\mathbb{E}(\epsilon|X,\eta) = 0$. Now assume that the instrument $Z$ satisfies
$$
\mathbb{E}(Y|X,\eta,Z) = \mathbb{E}(Y|X,\eta) \quad \text{and} \quad \mathbb{E}(\eta|Z) = \tau.
$$
Then it follows that 
$$
\begin{align*}
\mathbb{E}(\epsilon|X,Z,\eta) &= \mathbb{E}[Y - \exp(\alpha + \beta X) \eta | X,Z,\eta] = \mathbb{E}(Y|X,Z,\eta) - \exp(\alpha + \beta X) \eta \\
&= \mathbb{E}(Y|X,\eta) - \mathbb{E}(Y|X,\eta) = 0
\end{align*}
$$
by the exclusion restriction from above. Now define
$$
T(Y,X;\alpha, \beta) = Y / \exp(\alpha + \beta X).
$$
Then we have
$$
\begin{align}
\mathbb{E}[T(Y,X;\alpha, \beta) - 1 | Z] &= \mathbb{E}\left[\left.\frac{Y}{\exp(\alpha + \beta X)}  - 1 \right| Z\right] \\
&= \mathbb{E}\left[ \left.\frac{\exp(\alpha + \beta X)\eta + \epsilon}{\exp(\alpha + \beta X)} - 1\right| Z\right] \\
&= \mathbb{E}[\eta - 1|Z] + \mathbb{E}\left[\left. \frac{\epsilon}{\exp(\alpha + \beta X)} \right|Z\right]\\
&= (\tau - 1) + \mathbb{E}\left[ \frac{1}{\exp(\alpha + \beta X)} \mathbb{E}\left(\left. \epsilon \right| X, Z, \eta\right)\right]\\
&= (\tau - 1) + \mathbb{E}\left[ \frac{1}{\exp(\alpha + \beta X)} \times 0\right]\\
&= (\tau - 1)
\end{align}
$$
And since we have an intercept $\alpha$ we can normalize $\tau$ to one so the moment condition becomes zero. Therefore, the conditional moment equalities are
$$
\mathbb{E}\left[ \left.\frac{Y}{\exp(\alpha + \beta X)} - 1 \right| Z\right] = 0.
$$
To carry out estimation, we can convert this into a collection of unconditional moment equalities by multiplying the the moment function by any function of $Z$:
$$
\mathbb{E}\left[ \left\{\frac{Y}{\exp(\alpha + \beta X)} - 1 \right\} \varphi(Z)\right] = 0.
$$
An obvious choice here would be $\varphi(Z) = (1, Z)$, so the moment equations would be
$$
\begin{align*}
\mathbb{E}\left[ \left\{\frac{Y}{\exp(\alpha + \beta X)} - 1 \right\} \right] &= 0\\
\mathbb{E}\left[ \left\{\frac{Y}{\exp(\alpha + \beta X)} - 1 \right\} Z\right] &= 0.
\end{align*}
$$
This is a two-dimensional non-linear optimization problem. Solving the first moment equation for $\alpha$ gives
$$
\alpha = \log \mathbb{E}[Y / \exp(\beta X)].
$$
In other words, $\exp(\alpha) = \mathbb{E}[Y/\exp(\beta X)]$. This allows us to "concentrate out" $\alpha$ from the second equation to obtain a one-dimensional root finding problem. Since this works for any $\varphi(Z)$ function, I'll present the general case:
$$
\mathbb{E}\left[ \left\{ \frac{Y}{\mathbb{E}[Y/\exp(\beta X)] \exp(\beta X) } - 1 \right\} \varphi(Z)\right] = 0.
$$
We can simplify this as to become
$$
\frac{1}{\mathbb{E}[Y/\exp(\beta X)]}\mathbb{E}\left[ \left\{ \frac{Y}{\exp(\beta X) } - \mathbb{E}\left[\frac{Y}{\exp(\beta X)}\right] \right\} \varphi(Z)\right] = 0.
$$
But since the first factor in the product can never be zero, the moment equation simplifies further to
$$
\mathbb{E}\left[ \left\{ \frac{Y}{\exp(\beta X) } - \mathbb{E}\left[\frac{Y}{\exp(\beta X)}\right] \right\} Z\right] = 0.
$$
Notice that, by the definition of covariance, this is simply
$$
\text{Cov}\left(\frac{Y}{\exp(\beta X)}, \varphi(Z)\right) = 0.
$$
for any function of $Z$, in particular for $\varphi(Z) = Z$.

Let's test this out on the simulation data:
```{r}
#| error: true
f <- function(b) {
  with(dat, cov(y / exp(b * log(1 + x)), z))
}
f_vectorized <- Vectorize(f)
b_seq <- seq(-1, 1, 0.01)
plot(b_seq, f_vectorized(b_seq), type = 'l')
abline(h = 0, lty = 2, lwd = 2, col = 'red')
uniroot(f, c(-1, 1))
```

**Note:** We played around with various choices of parameters. For some of them there are multiple roots and for some there are no roots. The same goes for *fixed* parameters but setting a different random seed. When there are multiple roots, you can't seem to rule them out with additional moment conditions, i.e. by checking whether, say, the covariance with $z^2$ is also zero. Depending on the data it seems like you can end up in situations where the model is not identified from any particular set of conditional moment restrictions that you choose, even if it might be identified from the full set of unconditional moment restrictions.

<!-- Let's check the implied value of alpha
```{r}
#b_mullahy_1 <- uniroot(f, c(0, 1.2))$root
#with(dat, log(mean(y / exp(b_mullahy_1 * x)))) # implied alpha
#
#b_mullahy_2 <- uniroot(f, c(1.2, 2))$root
#with(dat, log(mean(y / exp(b_mullahy_2 * x)))) # implied alpha
```
Interesting: two roots, one of which is "correct" and another of which is wrong but not very far away! I wonder if this is one of those situations in which you can (possibly) show point identification using the full set of conditional moment restrictions but a *particular* choice of unconditional moment equations doesn't necessarily give you point identification even if the number of equations is greater than or equal to the number of unknowns.

Let's check whether an additional moment restriction would rule out the extraneous root by evaluating an "overidentifying restriction" at a different function of $Z$
```{r}
#with(dat, cov(y / exp(b_mullahy_1 * x), z^2))
#with(dat, cov(y / exp(b_mullahy_1 * x), z^3))
#
#with(dat, cov(y / exp(b_mullahy_2 * x), z^2))
#with(dat, cov(y / exp(b_mullahy_2 * x), z^3))
```
-->


## Bayesian Estimation

```{r}
library(cmdstanr)
model1 <- cmdstan_model('mullahy-version1.stan')
model1$print()

d <- list(x = dat$x,
          y = dat$y,
          z = dat$z,
          N = nrow(dat))

fit1 <- model1$sample(
  data = d,
  seed = 5678,
  chains = 4,
  parallel_chains = 4,
  refresh = 500,
  #iter_warmup = 2000,
  #iter_sampling = 6000
)
```

It looks like it ran successfully! Now let's check the posterior:
```{r}
fit1$summary(variables = c('alpha', 'beta', 'gamma', 'delta', 'sigma_x', 'sigma_y')) |> knitr::kable(digits = 2)
```

For purposes of comparison, let's try a version with a non-centered parameterization. Notice that the interpretation of the parameters in the "first stage" changes here, but these aren't the parameters of interest in any case
```{r}
model2 <- cmdstan_model('mullahy-version2.stan')
model2$print()
```

As one might expect, the centered parameterization performs very poorly:
```{r}
fit2 <- model2$sample(
  data = d,
  seed = 5678,
  chains = 4,
  parallel_chains = 4,
  refresh = 500,
)
fit2$summary(variables = c('alpha', 'beta', 'rho', 'delta', 'sigma_u', 'eta')) |> knitr::kable(digits = 2)
```








