---
title: "endogenous-poisson-normal-mixture"
format: 
  html:
    embed-resources: true
---

## The Model 
$$
\begin{align}
Y_i & \sim \text{Poisson}(\lambda_i)\\
X_i & \sim \text{Poisson}(\mu_i) \\
\log(\lambda_i) &= \alpha + \beta X_i + U_i \\
\log(\mu_i) &= \gamma + \delta Z_i + V_i \\
\begin{bmatrix}
U_i \\ V_i
\end{bmatrix} &\sim \text{Normal}( \mathbf{0}, \Sigma)\\
\Sigma &\equiv 
\begin{bmatrix} \sigma_U^2 & \sigma_U \sigma_V \rho \\
\sigma_U \sigma_V \rho & \sigma_V^2\end{bmatrix}
\end{align}
$$
```{r}
#| warning: false
#| message: false
library(rethinking)
library(tidyverse)
library(patchwork)
library(mvtnorm)

set.seed(2817)
n <- 2500
rho <- -0.8
sV <- 1
sU <- 1 
S <- matrix(c(sU^2, rho * sU * sV, 
              rho * sU * sV, sV^2), 2, 2, byrow = TRUE)

errors <- rmvnorm(n, sigma = S)

a <- 0
b <- 0.15
g <- 0
d <- 0.3


sim_dat <- tibble(u = errors[, 1],
                  v = errors[, 2],
                  z = runif(n, -0.3, 0.3),
                  x = rpois(n, exp(g + d * (z - mean(z)) / sd(z) + v)),
                  y = rpois(n, exp(a + b * (x - mean(x)) / sd(x) + u)))


dat <- sim_dat |> 
  select(x, y, z)

y_z <- dat |> 
  ggplot(aes(x = z, y = y)) +
  geom_point()

y_x <- dat |> 
  ggplot(aes(x = x, y = y)) +
  geom_point()

x_z <- dat |> 
  ggplot(aes(x = z, y = x)) +
  geom_point()

y_z + y_x + x_z
```
## How much endogeneity have we created?
```{r}
library(broom)
glm(y ~ x, family = poisson, data = dat) |> 
  tidy() |> 
  knitr::kable(digits = 2)
```


## Mullahy Approach

Suppose that
$$
\mathbb{E}(Y|X,\eta) = \exp(\alpha + \beta X) \eta = \exp(\alpha + \beta X + U)
$$
where $U \equiv \log(\eta)$. Then we can always write 
$$
Y = \mathbb{E}(Y|X,\eta) + \epsilon = \exp(\alpha + \beta X) \eta + \epsilon
$$
where $\mathbb{E}(\epsilon|X,\eta) = 0$. Now assume that the instrument $Z$ satisfies
$$
\mathbb{E}(Y|X,\eta,Z) = \mathbb{E}(Y|X,\eta) \quad \text{and} \quad \mathbb{E}(\eta|Z) = \tau.
$$
Then it follows that 
$$
\begin{align*}
\mathbb{E}(\epsilon|X,Z,\eta) &= \mathbb{E}[Y - \exp(\alpha + \beta X) \eta | X,Z,\eta] = \mathbb{E}(Y|X,Z,\eta) - \exp(\alpha + \beta X) \eta \\
&= \mathbb{E}(Y|X,\eta) - \mathbb{E}(Y|X,\eta) = 0
\end{align*}
$$
by the exclusion restriction from above. Now define
$$
T(Y,X;\alpha, \beta) = Y / \exp(\alpha + \beta X).
$$
Then we have
$$
\begin{align}
\mathbb{E}[T(Y,X;\alpha, \beta) - 1 | Z] &= \mathbb{E}\left[\left.\frac{Y}{\exp(\alpha + \beta X)}  - 1 \right| Z\right] \\
&= \mathbb{E}\left[ \left.\frac{\exp(\alpha + \beta X)\eta + \epsilon}{\exp(\alpha + \beta X)} - 1\right| Z\right] \\
&= \mathbb{E}[\eta - 1|Z] + \mathbb{E}\left[\left. \frac{\epsilon}{\exp(\alpha + \beta X)} \right|Z\right]\\
&= (\tau - 1) + \mathbb{E}\left[ \frac{1}{\exp(\alpha + \beta X)} \mathbb{E}\left(\left. \epsilon \right| X, Z, \eta\right)\right]\\
&= (\tau - 1) + \mathbb{E}\left[ \frac{1}{\exp(\alpha + \beta X)} \times 0\right]\\
&= (\tau - 1)
\end{align}
$$
And since we have an intercept $\alpha$ we can normalize $\tau$ to one so the moment condition becomes zero. Therefore, the conditional moment equalities are
$$
\mathbb{E}\left[ \left.\frac{Y}{\exp(\alpha + \beta X)} - 1 \right| Z\right] = 0.
$$
To carry out estimation, we can convert this into a collection of unconditional moment equalities by multiplying the the moment function by any function of $Z$:
$$
\mathbb{E}\left[ \left\{\frac{Y}{\exp(\alpha + \beta X)} - 1 \right\} \varphi(Z)\right] = 0.
$$
An obvious choice here would be $\varphi(Z) = (1, Z)$, so the moment equations would be
$$
\begin{align*}
\mathbb{E}\left[ \left\{\frac{Y}{\exp(\alpha + \beta X)} - 1 \right\} \right] &= 0\\
\mathbb{E}\left[ \left\{\frac{Y}{\exp(\alpha + \beta X)} - 1 \right\} Z\right] &= 0.
\end{align*}
$$
This is a two-dimensional non-linear optimization problem. Solving the first moment equation for $\alpha$ gives
$$
\alpha = \log \mathbb{E}[Y / \exp(\beta X)].
$$
In other words, $\exp(\alpha) = \mathbb{E}[Y/\exp(\beta X)]$. This allows us to "concentrate out" $\alpha$ from the second equation to obtain a one-dimensional root finding problem. Since this works for any $\varphi(Z)$ function, I'll present the general case:
$$
\mathbb{E}\left[ \left\{ \frac{Y}{\mathbb{E}[Y/\exp(\beta X)] \exp(\beta X) } - 1 \right\} \varphi(Z)\right] = 0.
$$
We can simplify this as to become
$$
\frac{1}{\mathbb{E}[Y/\exp(\beta X)]}\mathbb{E}\left[ \left\{ \frac{Y}{\exp(\beta X) } - \mathbb{E}\left[\frac{Y}{\exp(\beta X)}\right] \right\} \varphi(Z)\right] = 0.
$$
But since the first factor in the product can never be zero, the moment equation simplifies further to
$$
\mathbb{E}\left[ \left\{ \frac{Y}{\exp(\beta X) } - \mathbb{E}\left[\frac{Y}{\exp(\beta X)}\right] \right\} Z\right] = 0.
$$
Notice that, by the definition of covariance, this is simply
$$
\text{Cov}\left(\frac{Y}{\exp(\beta X)}, \varphi(Z)\right) = 0.
$$
for any function of $Z$, in particular for $\varphi(Z) = Z$.

Let's test this out on the simulation data:
```{r}
f <- function(b) {
  with(dat, cov(y / exp(b * x), z))
}
f_vectorized <- Vectorize(f)
b_seq <- seq(-1, 1, 0.01)
plot(b_seq, f_vectorized(b_seq), type = 'l')
abline(h = 0, lty = 2, lwd = 2, col = 'red')
uniroot(f, c(-1, 1))
```

**Note:** We played around with various choices of parameters. For some of them there are multiple roots and for some there are no roots. The same goes for *fixed* parameters but setting a different random seed. When there are multiple roots, you can't seem to rule them out with additional moment conditions, i.e. by checking whether, say, the covariance with $z^2$ is also zero. Depending on the data it seems like you can end up in situations where the model is not identified from any particular set of conditional moment restrictions that you choose, even if it might be identified from the full set of unconditional moment restrictions.

<!-- Let's check the implied value of alpha
```{r}
b_mullahy_1 <- uniroot(f, c(0, 1.2))$root
with(dat, log(mean(y / exp(b_mullahy_1 * x)))) # implied alpha

b_mullahy_2 <- uniroot(f, c(1.2, 2))$root
with(dat, log(mean(y / exp(b_mullahy_2 * x)))) # implied alpha
```
Interesting: two roots, one of which is "correct" and another of which is wrong but not very far away! I wonder if this is one of those situations in which you can (possibly) show point identification using the full set of conditional moment restrictions but a *particular* choice of unconditional moment equations doesn't necessarily give you point identification even if the number of equations is greater than or equal to the number of unknowns.

Let's check whether an additional moment restriction would rule out the extraneous root by evaluating an "overidentifying restriction" at a different function of $Z$
```{r}
with(dat, cov(y / exp(b_mullahy_1 * x), z^2))
with(dat, cov(y / exp(b_mullahy_1 * x), z^3))

with(dat, cov(y / exp(b_mullahy_2 * x), z^2))
with(dat, cov(y / exp(b_mullahy_2 * x), z^3))
```
-->


## Bayesian Estimation

This doesn't work and I don't know why:
```{r}
#| warning: false
d <- dat |> 
  rowid_to_column('id') |>  # Calling it i throws a STAN error: it uses i internally
  select(x, y, z, id)

iv_pois <- ulam(
  alist(
    y ~ dpois(lambda),
    x ~ dpois(mu),
    c(u[id], v[id]) ~ multi_normal(c(0, 0), Rho, Sigma),
    log(lambda) <- a + b * x + u[id],
    log(mu) <- g + d * z + v[id],
    c(a, g) ~ normal(0, 1.5),
    c(b, d) ~ normal(0, 0.2),
    Rho ~ lkj_corr(2),
    Sigma ~ exponential(1)
  ), data = d, chains = 4, cores = 4, cmdstan = TRUE
)
```

```{r}
#| warning: false
#m14.6 <- ulam(
#  alist(
#    c(W, E) ~ multi_normal(c(muW, muE), Rho, Sigma),
#    muW <- aW + bEW * E,
#    muE <- aE + bQE * Q, 
#    c(aW, aE) ~ normal(0, 0.2), # indep priors
#    c(bEW, bQE) ~ normal(0, 0.5), # indep priors
#    Rho ~ lkj_corr(2), 
#    Sigma ~ exponential(1)),
#  data = dat_sim, chains = 4, cores = 4)
#precis(m14.6, depth = 3)
```















