---
title: "Bayesian IV with STAN"
format: 
  html:
    embed-resources: true
---

## The Simplest Possible Example
I'll start with the example from `instrumental-variables.qmd` that is based on *Statistical Rethinking* 14-6. For the moment I'll use the same notation as the book to avoid confusion. Then I'll switch over to more "familiar" IV notation. In the example, $W$ is wage, $E$ is years of education and $Q$ is quarter of birth:
$$
\begin{align*}
\begin{bmatrix}
W \\ E
\end{bmatrix} &\sim \text{Normal}\left(\begin{bmatrix}\mu_W \\ \mu_E \end{bmatrix}, \mathbf{S}\right)\\
\mu_W &= \alpha_W + \beta_{EW} E\\
\mu_E &= \alpha_E + \beta_{QE} Q
\end{align*}
$$
In the true DGP there is no effect of education on wage, and quarter of birth is sampled uniformly from `1:4`
```{r}
#| warning: false
#| message: false
library(rethinking)
set.seed(73)
N <- 500
U_sim <- rnorm(N)
Q_sim <- sample(1:4, size = N, replace = TRUE) # quarter of birth
E_sim <- rnorm(N, U_sim + Q_sim) # education
W_sim <- rnorm(N, U_sim + 0 * E_sim) # wage
```
No now let's look at how this relates to the more "standard" way of writing an IV model, namely:
$$
\begin{align*}
Y_i &= \beta_0 + \beta_1 X_i + U_i \\
X_i &= \pi_0 + \pi_1 Z_i + V_i\\
\begin{pmatrix}
U_i \\ V_i
\end{pmatrix} &\sim\text{Normal}\left( \begin{bmatrix} 0 \\ 0\end{bmatrix}, 
\begin{bmatrix}
\sigma_U^2 & \sigma_U \sigma_V \rho \\
\rho \sigma_U \sigma_V & \sigma_V^2
\end{bmatrix}
\right).
\end{align*}
$$
Besides the choice of variable and parameter names, the main difference between these two is that *Statistical Rethinking* states the distribution for the observed variables $(W, E)$ *directly* whereas the more "econometrics" way of writing things states the distribution for the *unobserved* variables. 


**Ok so basically the point is that in the rethinking version $U$ is the unobserved confounder, and the structural errors are only implied. I need to think about how this relates to the right way of specifying the model.**


*Statistical Rethinking* estimates most models with centered and standardized variables, to make it easier to think of reasonable priors. This one is no exception. But since the true effect of education is zero on the "raw" scale, so is the effect on the standardized scale. 
```{r}
dat_sim <- list(
  W = standardize(W_sim), 
  E = standardize(E_sim),
  Q = standardize(Q_sim)
)
```
Here the choice of priors really merits a bit of additional discussion. I'll return to that below. First I'll simply copy the choices from *Statistical Rethinking* without comment to make sure that I can implement the model in STAN and obtain reasonable results. The STAN code generated by `ulam()` is as follows:
```{r}
m14_6 <- ulam(
  alist(
    c(W, E) ~ multi_normal(c(muW, muE), Rho, Sigma),
    muW <- aW + bEW * E,
    muE <- aE + bQE * Q, 
    c(aW, aE) ~ normal(0, 0.2), # indep priors
    c(bEW, bQE) ~ normal(0, 0.5), # indep priors
    Rho ~ lkj_corr(2), 
    Sigma ~ exponential(1)
  ), data = dat_sim, sample = FALSE
)
stancode(m14_6)
```

## How does this related 

## Different Approaches 
I seem to recall that there are two different thoughts on how to set up priors for this kind of models. (Chamberlain & Imbens talk about this and I think the Bayesian Statistics in Marketing book does as well.) One approach puts priors on the *structural* parameters, while the other puts priors on the *reduced form* parameters. **I need to come back and add some discussion of this**.

## More familiar notation
$$
\begin{align*}
Y_i &= \beta_0 + \beta_1 X_i + U_i \\
X_i &= \pi_0 + \pi_1 Z_i + V_i\\
\begin{pmatrix}
U_i \\ V_i
\end{pmatrix} &\sim\text{Normal}\left( \begin{bmatrix} 0 \\ 0\end{bmatrix}, 
\begin{bmatrix}
\sigma_U^2 & \sigma_U \sigma_V \rho \\
\rho \sigma_U \sigma_V & \sigma_V^2
\end{bmatrix}
\right)
\end{align*}
$$

## To Add Next
Here's a [more complicated](https://khakieconomics.github.io/2017/11/26/Bayesian_iv.html) model with a hierarchical prior. This seems very closely related to a little-known but very interesting paper of [Chamberlain & Imbens](https://www.nber.org/papers/t0204) that (as far as I can tell) was later re-packaged without a self-citation to the earlier version into a [random coefficients paper](https://doi.org/10.1111/j.1468-0262.2004.00485.x) that seems to do almost exactly the same thing without mentioning the word "Bayesian" anywhere. Next I'd like to implement the earlier Chamberlain & Imbens paper *exactly* using STAN and then consider a more reasonable choice of priors for the covariance matrices, e.g. LKJ.