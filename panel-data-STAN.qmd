---
title: "Panel Data with STAN"
format: 
  html:
    embed-resources: true
---

## Linear Panel Example

Below is a more complicated example from *Statistical Rethinking* 2023, in which the outcome of interest is binary. When estimating those models, we encountered some slightly strange results, in that the FE and RE specifications give very similar estimates, so we decided to test out linear panel models with the same structure to make sure our intuition is reliable. The model is 
$$
Y_{it} = \alpha + \beta X_{it} + \gamma Z_i + U_i + \epsilon_{it}
$$
where $U_i$ is an individual effect that is correlated with $X_{it}$ and $\epsilon_{it}$ is an idiosyncratic effect that is independent of $X_{it}$ and $Z_i$:
```{r}
Nt <- 3
Ni <- 250
N <- Nt * Ni
alpha <- (-2)
beta <- 1
gamma <- (-0.5)
sigma <- 1

set.seed(1693)

# Vector of individual ids in the full panel 
id <- rep(1:Ni, each = Nt)

# Variables that are fixed over time 
U <- rnorm(Ni, 1.5) # unobserved
Z <- rnorm(Ni) # observed

# Variables that vary across i and t
X <- rnorm(N, mean = U[id])
epsilon <- rnorm(N, sd = sigma)
Y <- alpha + beta * X + gamma * Z[id] + U[id] + epsilon 
```

Now let's start by running the *usual* frequentist fixed effects and random effects estimators:
```{r}
library(tidyverse)
library(plm)
library(fixest)
library(modelsummary)
mypanel <- tibble(y = Y, x = X, z = Z[id], id = factor(id))
random_effects <- plm(y ~ x + z, data = mypanel, model = 'random', index = 'id')
modelsummary(list(OLS = lm(y ~ x, mypanel),
                  RE = random_effects, 
                  FE = feols(y ~ x | id, mypanel)),
             gof_omit = 'AIC|BIC|F|RMSE|R2|Log.Lik.')
```

So in this example, the fixed effects estimator does a good job estimating the effect of $X_{it}$ where the random effects estimator is severly biased, almost a much as OLS. Now we'll try the Bayesian versions of the same to see how they compare.
```{r}
library(cmdstanr)

linear_FE_model <- cmdstan_model('panel-simple-linear-FE.stan')
linear_FE_model$print()

linear_RE_model <- cmdstan_model('panel-simple-linear-RE.stan')
linear_RE_model$print()
```

And now we can fit these models:
```{r}
dat <- list(N = N, Ni = Ni, Y = Y, Z = Z, X = X, id = id)

linear_FE_fit <- linear_FE_model$sample(
  data = dat,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  refresh = 500
)

linear_RE_fit <- linear_RE_model$sample(
  data = dat,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  refresh = 500
)
```

And summarize them:
```{r}
linear_FE_fit$summary(variables = c('beta', 'gamma', 'sigma')) |> 
  knitr::kable(digits = 2)

linear_RE_fit$summary(variables = c('beta', 'gamma', 'tau', 'sigma', 'abar')) |> 
  knitr::kable(digits = 2)
```




## Example from *Statistical Rethinking 2023*

This example is based on [Lecture 12](https://youtu.be/iwVqiiXYeC4?feature=shared&t=3291) of the 2023 video lecture series to accompany the book *Statistical Rethinking*. The example itself is not in the most recent version of the book, although I believe it is slated to appear in the next version.

### The Model

This is a logit regression with correlated individual effects. Let $g = 1, ..., G$ index groups and $i = 1, ... N_g$ index individuals within groups. There are $N \equiv \sum_{g=1}^G N_g$ individuals divided across $G = 30$ groups of different sizes $N_g$. There is a group-level observable $Z_g$ and a group-level unobservable $U_g$. The individual-level observables are $X_{ig}$ and $Y_{ig}$. The models is as follows:
$$
\mathbb{P}(Y_{ig} = 1|X_{ig}, Z_g, U_g) = \texttt{plogis}(\alpha + \beta X_{ig} + \gamma 
Z_g + U_g).
$$
First we'll simulate some data from the model as follows:
```{r}
N_groups <- 30
N_id <- 200
alpha <- (-2)
beta <- 1
gamma <- (-0.5)

set.seed(1693)

# Vector of group indicators for each individual
group_id <- sample(1:N_groups, size = N_id, replace = TRUE)

# Group-level variables
U <- rnorm(N_groups, 1.5) # unobserved
Z <- rnorm(N_groups) # observed

# Individual-level variables
X <- rnorm(N_id, mean = U[group_id])
P <- plogis(alpha + beta * X + gamma * Z[group_id] + U[group_id])
Y <- rbinom(N_id, size = 1, prob = P)
```

### "Fixed Effects" Model
This is a tiny bit different from a standard fixed effects model, in that there's a weakly informative prior on each of the fixed effects instead of a completely flat one. But there's no *hierarchy* so information isn't shared across groups to estimate the respective fixed effects. Here the effect of the group-level observable cannot be identified, so the prior and posterior should coincide.
$$
\begin{align*}
Y_{ig} & \sim \text{Bernoulli}(p_{ig})\\
\text{logit}(p_i) &= \alpha_g + \beta X_{ig} + \gamma Z_g\\
\alpha_g &\sim \text{Normal}(0, 10)\\
\beta, \gamma &\sim \text{Normal}(0, 1) \\
\end{align*}
$$

```{r}
#| message: FALSE
#| warning: FALSE
library(rethinking)

dat <- list(Y = Y, X = X, Z = Z, g = group_id, Ng = N_groups)
mf <- ulam(
  alist(
    Y ~ bernoulli(p),
    logit(p) <- alpha[g] + beta * X + gamma * Z[g],
    alpha[g] ~ dnorm(0, 10),
    beta ~ dnorm(0, 1),
    gamma ~ dnorm(0, 1)
  ), data = dat, sample = FALSE
)
stancode(mf)
```


Here's our slightly simplified and improved STAN implementation:
```{r}
#| warning: false
#| message: false
library(cmdstanr)
fe_model <- cmdstan_model('rethinking-panel-FE.stan')
fe_model$print()
```
Now we'll try it out on the simulation data:
```{r}
dat$Ni <- N_id
fe_fit <- fe_model$sample(
  data = dat,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  refresh = 500
)
fe_fit$summary(variables = c('beta', 'gamma')) |> 
  knitr::kable(digits = 2)
```


### "Random Effects" Model
This is really a multilevel model, in that we treat the varying intercepts as arising from a common distribution.
$$
\begin{align*}
Y_{ig} & \sim \text{Bernoulli}(p_{ig})\\
\text{logit}(p_i) &= \alpha_g + \beta X_{ig} + \gamma Z_g\\
\alpha_g &\sim \text{Normal}(\bar{\alpha}, \tau)\\
\bar{\alpha} &\sim \text{Normal}(0, 1)\\
\tau &\sim \text{Exponential}(1)\\
\beta, \gamma &\sim \text{Normal}(0, 1) \\
\end{align*}
$$

```{r}
mr <- ulam(
  alist(
    Y ~ bernoulli(p),
    logit(p) <- alpha[g] + beta * X + gamma * Z[g],
    transpars> vector[Ng]:a <<- abar + tau * epsilon,
    epsilon[g] ~ dnorm(0, 1),
    c(beta, gamma) ~ dnorm(0, 1),
    tau ~ dexp(1)
  ), data = dat, sample = FALSE
)
stancode(mr)
```

Here's our slightly improved version:
```{r}
re_model <- cmdstan_model('rethinking-panel-RE.stan')
re_model$print()
```

And now let's test it out:
```{r}
re_fit <- re_model$sample(
  data = dat,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  refresh = 500
)
re_fit$summary(variables = c('beta', 'gamma')) |> 
  knitr::kable(digits = 2)
```


### "Fast and Dirty" Mundlak
Let $\bar{X}_g \equiv \frac{1}{N_g} \sum_{i=1}^{N_g} X_{ig}$ be the within-group mean of $X_{ig}$. The "fast and dirty" Mundlak approach, simply adds this as an additional predictor in the multilevel ("random effects") model from above.

$$
\begin{align*}
Y_{ig} & \sim \text{Bernoulli}(p_{ig})\\
\text{logit}(p_i) &= \alpha_g + \beta X_{ig} + \gamma Z_g + \delta \bar{X}_g\\
\alpha_g &\sim \text{Normal}(\bar{\alpha}, \tau)\\
\bar{\alpha} &\sim \text{Normal}(0, 1)\\
\tau &\sim \text{Exponential}(1)\\
\beta, \gamma, \delta &\sim \text{Normal}(0, 1) \\
\end{align*}
$$

```{r}
#| warning: FALSE
library(tidyverse)

dat$Xbar <- tibble(X = X, g = group_id) |> 
  group_by(g) |> 
  summarize(Xbar = mean(X)) |> 
  pull(Xbar)

mrx <- ulam(
  alist(
    Y ~ bernoulli(p),
    logit(p) <- alpha[g] + beta * X + gamma * Z[g] + delta * Xbar[g],
    transpars> vector[Ng]:a <<- abar + tau * epsilon,
    epsilon[g] ~ dnorm(0, 1),
    c(beta, gamma, delta) ~ dnorm(0, 1),
    tau ~ dexp(1)
  ), data = dat, sample = FALSE
)
stancode(mrx)
```


### "Full Luxury" Mundlak

This model accounts for the fact that $\bar{X}_g$ is not in fact known but must be estimated from data. To do this, it introduces a model for $X_{ig}$ in terms of an unobserved latent group-level variable $U_g$. This is modeled as standard normal.  
$$
\begin{align*}
Y_{ig} & \sim \text{Bernoulli}(p_{ig})\\
\text{logit}(p_i) &= \alpha_g + \beta X_{ig} + \gamma Z_g + \delta U_g\\
\alpha_g &\sim \text{Normal}(\bar{\alpha}, \tau)\\
\bar{\alpha} &\sim \text{Normal}(0, 1)\\
\tau &\sim \text{Exponential}(1)\\
\beta, \gamma, \delta &\sim \text{Normal}(0, 1) \\ \\
X_{ig} &\sim \text{Normal}(\mu_g, \sigma)\\
\mu_g &= \lambda + \kappa U_g\\
\lambda &\sim \text{Normal}(0, 1)\\
\kappa, \sigma & \sim \text{Exponential}(1)\\
U_g &\sim \text{Normal}(0,1)
\end{align*}
$$

```{r}
mru <- ulam(
  alist(
    # Y model 
    Y ~ bernoulli(p),
    logit(p) <- alpha[g] + beta * X + gamma * Z[g] + delta * U[g],
    transpars> vector[Ng]:a <<- abar + tau * epsilon,
    
    # X model
    X ~ normal(mu, sigma),
    mu <- lambda + kappa * u[g],
    vector[Ng]:u ~ normal(0, 1),
      
    # Priors
    epsilon[g] ~ dnorm(0, 1),
    c(lambda, beta, gamma, delta) ~ dnorm(0, 1),
    tau ~ dexp(1),
    sigma ~ dexp(1),
    kappa ~ dexp(1)
  ), data = dat, sample = FALSE
)
stancode(mru)
```

