---
title: "Poisson Regression with STAN"
format: 
  html:
    embed-resources: true
---

## What is this?
In this document, I implement the Poisson regression models from `poisson-regression.qmd` that were implemented using `ulam()` from the `rethinking` package, but now I use STAN instead. I also re-implement the Poisson-normal mixture from `poisson-normal-mixture.qmd`, again using STAN rather than `ulam()`. 



## Plain Vanilla Poisson Regression
OK, this isn't one of the examples from `poisson-regression.qmd` but I wanted to start with the simplest possible example!
$$
\begin{align*}
Y_i &\sim \text{Poisson}(\lambda_i)\\
\log (\lambda_i) &= \alpha + \beta (X_i - \bar{X})/S_X\\
\alpha &\sim \text{Normal}(3, 0.5)\\
\beta &\sim \text{Normal}(0, 0.2)
\end{align*}
$$
These are reasonable weakly informative priors in a setting where counts are generally between 0 and 100, given that $X_i$ enters the model as a z-score.

Now, we'll follow procedure outlined in `CmdStanR-getting-started.qmd`. First, load `cmdstanr` and check that everything is set up correctly:
```{r}
library(cmdstanr)
check_cmdstan_toolchain()
```

Next compile the first model, which is stored in `poisson-regression-basic.stan` in the current working directory. Since this directory is associated with an Rstudio project, there is no need to provide an absolute path:
```{r}
basic_poisreg <- cmdstan_model('poisson-regression-basic.stan')
```
Now we can look at the path to the executable:
```{r}
basic_poisreg$exe_file()
```
and print out the underlying `.stan` file:
```{r}
basic_poisreg$print()
```
It's worth commenting on this a bit, since it took me a while to get it working. (Fortunately, STAN gives relatively informative error messages!) Here are some errors that I made initially:

- A STAN `vector` can only store real values, but an `array` can store anything, including integers that are bounded.
- Rstudio uses `rstan` for its `.stan` [syntax checking](https://github.com/rstudio/rstudio/issues/6802), but the version of `rstan` that I installed from `CRAN` was out of date, so it didn't support the `array` type. This meant that the editor highlighted an "error" that was not in fact an error, and would not have created any problems given that I'm using CmdStanR. To fix this I simply installed the latest version of `rstan` following [these instructions](https://discourse.mc-stan.org/t/parser-error-when-running-birats-example-in-rstan/29336).
- Forgetting a semicolon! So easy to miss!
- Getting confused about how you can and cannot mix matrix / vector / scalar [operations](https://mc-stan.org/docs/functions-reference/matrix-arithmetic-operators.html).
- Forgetting to check the types that are expected by [`poisson_log_glm()`](https://mc-stan.org/docs/functions-reference/poisson-log-glm.html). This function is only worth using if you have more than one predictor.
- Not realizing that a `transformed data` block must come [before](https://mc-stan.org/docs/reference-manual/overview-of-stans-program-blocks.html) the `parameters` block.

And here are some other comments on the `.stan` file:

- The [`poisson_log()` function](https://mc-stan.org/docs/functions-reference/poisson-distribution-log-parameterization.html) avoids the need to exponentiate `alpha + beta * x`. In other words, it lets us express the model on the log scale. This is mainly for convenience, but I think it actually does squeeze out a bit of efficiency when it comes to calculating gradients. Its big brother [`poisson_log_glm()`](https://mc-stan.org/docs/functions-reference/poisson-log-glm.html) can *definitely* give efficiency gains. This removes the need for multiplication of `x` by `beta` but is really intended for the case where there are multiple predictors and the multiplication in question is matrix multiplication. That's where there are efficiency gains in the gradient calculation.
- It doesn't matter whether the "likelihood" or the "priors" come first in the model block. They are both treated the same way, since this block merely [increments the log posterior](https://discourse.mc-stan.org/t/correct-ordering-of-lines-in-the-model-block/661/3), and it doesn't matter in which order you take a summation, unless there are numerical issues of some kind.
- The `transformed data` block isn't necessary, but it's convenient. We could transform $X$ within R, but this prevents us from making a mistake and providing data on the wrong scale without realizing it.

Now let's simulate some data: 
```{r}
set.seed(1848)
n <- 100
x <- runif(n, -1, 2)
y <- rpois(n, exp(0.8 + 0.4 * scale(x)))
dat <- list(N = n, x = x, y = y)
rm(n, x, y)
```
and then estimate the model:
```{r}
fit1 <- basic_poisreg$sample(
  data = dat, 
  seed = 1234, # random seed for MCMC
  chains = 4, 
  parallel_chains = 4, 
  refresh = 500 # print status update after every 500 iterations
)
```

Now a quick summary of the posterior to make sure it worked. Recall that the true parameter values were $\alpha = 0.8$ and $\beta = 0.4$. Looks good!
```{r}
fit1$summary() |> 
  knitr::kable(digits = 2)
```


## Varying-Effects Poisson Regression

This is a slightly more complicated version of the above, based on an example from *Statistical Rethinking*. The goal is to model the total number of tools in a society, `total_tools`, in terms of `population` and the extent of contact with other islands, `contact`. The first model is as follows:
$$
\begin{align*}
T_i &\sim \text{Poisson}(\lambda_i)\\
\log \lambda_i &= \alpha_{\text{CID}[i]} + \beta_{\text{CID}[i]} \text{(logPopZ)}_i\\
\alpha_j &\sim \text{Normal}(3, 0.5)\\
\beta_j &\sim \text{Normal}(0, 0.2)
\end{align*}
$$
where $T_i$ is `total_tools`, $\text{(logPopZ)}_i$ is a centered and standardized version of the logarithm of `population` and $\text{CID}[i]$ is a categorical variable that indicates the value of `contact` for society $i$.
In other words, this is a model in which $\alpha$ and $\beta$ vary with `contact`.
Equivalently, the model includes a full set of dummy variables that encode `contact` along with interactions between these dummies and the log of `population`.

```{r}
#| message: false
#| warning: false
library(tidyverse)
library(rethinking)
data("Kline")
dat <- as_tibble(Kline)
rm(Kline)
dat

dat <- dat |> 
  mutate(cid = if_else(contact == 'low', 1, 2),
         lpop = log(population), 
         lpopz = (lpop - mean(lpop)) / sd(lpop)) |> 
  select(total_tools, cid, lpopz)
```

Let's start by looking at the STAN code that is automatically generated by `ulam()` from `rethinking` when we implement the model as described in the book. By setting `sample = FALSE` we set up the model without actually running anything. Then we can used `stancode()` to extract the underlying STAN code:
```{r}
m11_10 <- ulam(
  alist(
    total_tools ~ dpois(lambda),
    log(lambda) <- a[cid] + b[cid] * lpopz,
    a[cid] ~ dnorm(3, 0.5),
    b[cid] ~ dnorm(0, 0.2)
  ), data = dat, sample = FALSE 
)

stancode(m11_10)
```

This is fairly clear, but there are a few ways that it could be cleaned up and optimized, drawing on my simple Poisson regression example from above. The first is to both the number of observations and the number of contact indicators as data rather than hard-coding them to equal `10` and `2`. The second is to use `poisson_log()` rather than constructing `lambda` and exponentiating it. This is better for numerical stability. [This example](https://mc-stan.org/docs/stan-users-guide/hierarchical-logistic-regression.html) suggests some ways of eliminating `for` loops. The reason to avoid `for` loops is not that they're slow in STAN: everything compiles down to C++ so there's no speed penalty from loops as such. Instead it's an issue of more efficiently computing the gradients used in Hamiltonian MCMC. But in this case, there's only a single `for` loop and it doesn't seem straightforward to eliminate it: this is where the varying means $\lambda_i$ are constructed by indexing into the appropriate group for each observation. For this reason, I'll content myself with making only minor modifications, including transforming `population` within STAN:  

```{r}
m11_10_stan <- cmdstan_model('poisson-regression-rethinking-11-10.stan')
```
This time my STAN program compiled on the first try with no errors! Clearly I'm making progress. 
```{r}
m11_10_stan$print()
```

Now we'll set up the data to match the variable names I used in my STAN program and the fact that I transform `pop` within the program rather than doing so in advance:
```{r}
data("Kline")
dat <- list(N_obs = nrow(Kline),
            N_groups = length(unique(Kline$contact)),
            group = if_else(Kline$contact == 'low', 1, 2),
            pop = Kline$population,
            total_tools = Kline$total_tools)
rm(Kline)
```

Now we can estimate the model. The first time I ran this it threw an error: it turned out that I *did* have a mistake in my STAN program, but one that only became apparent at runtime: I declared one of the data arrays to be of the wrong size, and STAN caught this as soon as I passed in the data.
```{r}
fit2 <- m11_10_stan$sample(
  data = dat,
  seed = 1234,
  chains = 4, 
  parallel_chains = 4,
  refresh = 500
)
```
Now let's take a quick look at the posterior to see if it agrees with the results from `poisson-regression.qmd` computed via `ulam()`. Yes: everything looks good, bearing in mind that `precis()` reports 89\% rather than 90\% posterior credible intervals!
```{r}
fit2$summary() |> 
  knitr::kable(digits = 2)
```

## Structural Poisson Regression Model

This is the same tools/population example as above but with a structural model for the production of tools.
Suppose that the change in the average number of tools from one period to the next is given by
$$
\Delta T = \alpha P^\beta - \gamma T.
$$
where $P$ is population on the *raw* scale. The equilibrium number of tools is
$$
T^* = \frac{\alpha}{\gamma} P^\beta.
$$
This is a structural model.
To turn it into a *statistical* model, we need a likelihood i.e.\ a distribution for *noise* around $T^*$. 
Since `total_tools` is a count variable with no upper bound, the maximum entropy distribution is a Poisson. 
So we specify the model as follows
$$
\begin{align*}
T_i &\sim \text{Poisson}(\lambda_i) \\
\lambda_i &= \alpha_{\text{CID}[i]} P^{\beta_{\text{CID}[i]}}/\gamma.
\end{align*}
$$
Now we'll set up the data as in *Statistical Rethinking* and extract the STAN code from `ulam()`, using the same priors as the book:
```{r}
#| warning: false
#| message: false
data(Kline)
dat2 <- as_tibble(Kline) 
rm(Kline)
dat2 <- dat2 |> 
  mutate(cid = if_else(contact == 'low', 1, 2)) |> 
  select(population, cid, total_tools)

m11_11 <- ulam(
  alist(
    total_tools ~ dpois(lambda),
    lambda <- exp(a[cid]) * population^b[cid] / g,
    # Not sure why the book uses a normal and then exponentiates rather
    # than just using a lognormal. Maybe to avoid numerical issues?
    a[cid] ~ dnorm(1, 1),
    b[cid] ~ dexp(1),
    g ~ dexp(1)
  ), data = dat2, sample = FALSE
)
stancode(m11_11)
```
This is quite straightforward and very similar to the example from above. But just for fun, let's make a few small changes to the implementation. First, we'll express the model on the log scale: 
$$
\log(\lambda_i) = \log(\alpha_{\text{CID}[i]}) + \beta_{\text{CID}[i]}\log(P_i) - \log(\gamma).
$$
This makes it clear--if it wasn't already--that $\gamma$ is not identified in the Frequentist sense. Accordingly, define $\delta_{\text{CID}[i]} = \log(\alpha_{\text{CID}[i]}/\gamma)$, yielding the re-parameterized model
$$
\log(\lambda_i) = \delta_{\text{CID}[i]} + \beta_{\text{CID}[i]} \log(P_i).
$$
And since $\gamma$ isn't identified in any case, re-define $\alpha_{\text{CID}[i]}$ to be the ratio of the original parameter and $\gamma$ so that $\alpha_{\text{CID}[i]} = \exp(\delta_{CID}[i])$. We'll suppose that the parameter of interest is really $\alpha$ rather than $\delta$, giving us an opportunity to use a `transformed parameters` block in STAN. Note that our results here will not exactly correspond to those in the book, since we won't place a prior on the unidentified parameter $\gamma$.
```{r}
m11_11_stan <- cmdstan_model('poisson-regression-rethinking-11-11.stan') 
```

Here's the STAN program:
```{r}
m11_11_stan$print()
```

A few points worth noting:

- I explicitly set a lower bound of zero for $\beta$. This may seem strange given that the exponential prior already reflects this, but that's not actually how STAN works. There's some discussion on page 374 of *A Student's Guide to Bayesian Statistics*. In practice, if you have a parameter that is constrained you should impose this constraint in the `parameters` block.
- I use a `transformed parameters` block to construct the structural parameter $\alpha_\text{CID[i]}$ on its original non-log scale within STAN, rather than after the fact. See page 382 of *A Student's Guide to Bayesian Statistics*. No Jacobian is required for this, since we are transforming *after* sampling rather than before. See pages 398-401 of *A Student's Guide to Bayesian Statistics*. 
- I initially got a strange warning about "incomplete final line..." but it turns out that this is [harmless](https://discourse.mc-stan.org/t/incomplete-final-line-found/6907). To eliminate it, simply add a completely blank line to the end of the `.stan` file.

Now we can sample the model and look at the output. Remember that this model isn't quite the same as the one from the book, since I've changed the parameterization:
```{r}
data("Kline")
dat <- list(N_obs = nrow(Kline),
            N_groups = length(unique(Kline$contact)),
            group = if_else(Kline$contact == 'low', 1, 2),
            pop = Kline$population,
            total_tools = Kline$total_tools)
rm(Kline)

fit3 <- m11_11_stan$sample(
  data = dat,
  seed = 1234,
  chains = 4,
  parallel_chains = 4,
  refresh = 500
)

fit3$summary() |> 
  knitr::kable(digits = 2)
```

## Poisson Model with Offsets / Exposure

If there are $\lambda$ events per unit time, then in an interval of length $\tau$ there will be, on average, $\mu = \lambda \tau$ events.
If, as above, we have a linear model on the log scale for $\lambda_i$, then
$$
\log \mu_i = \log (\lambda_i \tau_i) = \log \tau_i + \log \lambda_i = \log \tau_i + \alpha + \beta x_i.
$$
In other words: to translate from a model for the *rate* $\lambda_i$ to a model for the *mean number of events* $\mu_i$, on the log scale we merely need to subtract $\log \tau_i$.

Here's a little simulation example based on: 11-12 from *Statistical Rethinking*.
There are two monasteries: the first has a rate of $\lambda = 1.5$ manuscripts per day and records daily totals; the second has a rate of $\lambda = 0.5$ manuscripts per day but records weekly totals.
```{r}
set.seed(54321)
n0 <- 100 # observe daily counts for 30 days 
y0 <- rpois(n0, lambda = 1.5) # rate of 1.5 manuscripts / day 

n1 <- 20 # observe weekly counts for 20 weeks
y1 <- rpois(n1, lambda = 7 * 0.5) # rate of 0.5 manuscripts / day times 7 days

manuscripts <- tibble(y = c(y0, y1),
                      monastery = c(rep(0, n0), rep(1, n1)), # monastery dummy
                      exposure = c(rep(1, n0), rep(7, n1)), # exposure in days
                      log_days = log(exposure))
```

For this example the book uses a Laplace approximation to the posterior rather than HMC. I'll use STAN and this time I'll try to write the model totally from scratch without looking at any examples! I'll also parameterize it differently, in keeping with the more usual practice with varying coefficient models.

**Important!** The first time I did this, I got incorrect results but no error messages! There was a very subtle error in the code and it took me a long time to figure out. Because this document is about learning STAN, I'll go through the mistake, show why it's wrong, and show how to correct it. Here's the `.stan` file for the **wrong version**, where I've included a print statement in the `transformed data` block that will reveal the error later on when we fit the model:

```{r}
m11_12_stan_WRONG <- cmdstan_model('poisson-offsets-rethinking-11-12-WRONG.stan')
m11_12_stan_WRONG$print()
```
Now we'll set up the data, and estimate the model. **Notice that the results are definitely wrong!** The credible set for `d` should be fairly tight and centered around one but it's not! We can see what's wrong from the debugging print statement: it's evaluating $\log(7)$ as one! In other words it's truncating!
```{r}
dat <- list(N_obs = nrow(manuscripts), 
            days = manuscripts$exposure,
            monastery = manuscripts$monastery + 1, # number them 1 and 2
            manuscripts = manuscripts$y)

fit4_WRONG <- m11_12_stan_WRONG$sample(
  data = dat,
  seed = 1234,
  chains = 4, 
  parallel_chains = 4,
  refresh = 500
)

fit4_WRONG$summary() |> 
  knitr::kable(digits = 2)
```

Just like in C++, `log()` applied to an integer in STAN returns another integer! This is easy to fix: simply store `pop` as `real` and everything works as expected:
```{r}
m11_12_stan <- cmdstan_model('poisson-offsets-rethinking-11-12.stan')
m11_12_stan$print()

fit4 <- m11_12_stan$sample(
  data = dat,
  seed = 1234,
  chains = 4, 
  parallel_chains = 4,
  refresh = 500
)

fit4$summary() |> 
  knitr::kable(digits = 2)
```
## Zero-inflated Poisson Model

Continuing with the Monks and manuscripts spiel, Example 12-3 from *Statistical Rethinking* fits a zero-inflated Poisson model of the form:
$$
\begin{align*}
Y_i &\sim \text{ZIPoisson}(p, \lambda)\\
\text{logit}(p)  &= \alpha_p \\
\log(\lambda)  &= \alpha_\lambda\\
\alpha_p &\sim \text{Normal}(-1.5, 1)\\
\alpha_\lambda &\sim \text{Normal}(1, 0.5)
\end{align*}
$$
The idea is that the monks take the day off and spend it drinking instead with probability $p$. When they drink, they produce zero manuscripts. When they don't drink, they produce a Poisson-distributed number of manuscripts, with rate $\lambda$. Hence, the distribution of $Y|p,\lambda$ is given by 
$$
\begin{align*}
\mathbb{P}(Y = y|p, \lambda) &= \mathbb{P}(Y=y|\text{Drink},p, \lambda) \mathbb{P}(\text{Drink}|p, \lambda) + \mathbb{P}(Y = y|\text{Work}, p, \lambda) \mathbb{P}(\text{Work}|p, \lambda)\\
&=  0 \times p + \frac{e^{-\lambda}\lambda^y}{y!} \times (1 - p)\\
&= (1 - p) \frac{\lambda^y \exp(-\lambda)}{y!}.
\end{align*}
$$
The indicator for whether or not the monks drink is an unobserved latent variable. Since the current version of STAN can't sample discrete parameters, we instead "integrate out" the latent variable.

Here's the simulated data and `ulam()` model
```{r}
p_true <- 0.2 # Monks drink on 20% of days
lambda_true <- 1 # When working, monasteries average 1 manuscript / day

n <- 365 # A year's worth of daily data on manuscript production

set.seed(365) # This is the seed used in the book

drink <- rbinom(n, 1, p_true) # indicator for whether the monks drink
y <- (1 - drink) * rpois(n, lambda_true) # manuscripts produce

m12_3 <- ulam(
  alist(
    y ~ dzipois(p, lambda),
    logit(p) <- ap,
    log(lambda) <- al,
    ap ~ dnorm(-1.5, 1),
    al ~ dnorm(1, 0.5)
  ), data = list(y = y), sample = FALSE
)

stancode(m12_3)
```

This is the first time we've encountered the `target += [SOMETHING]` syntax instead of a "sampling statment" of the form `[SOMETHING] ~ [DISTRIBUTION]([pars])` The notation `~` is convenient, but a bit misleading. Unlike JAGS and BUGS, STAN is an *imperative* rather than a *declarative* programming language. To understand what's going on, consider a vector of $N$ iid observations $\mathbf{Y}$ from a distribution with parameter $\theta$. Bayes theorem, without the normalizing constant, gives
$$
\begin{align*}
\pi(\theta|\mathbf{Y}) &\propto f(\mathbf{Y}|\theta) \times \pi(\theta) \\
&= \prod_{i=1}^N f(Y_i|\theta)\times \pi(\theta).
\end{align*}
$$
STAN works with the un-normalized *log posterior*, namely:
$$
\begin{align*}
\log \pi(\theta|\mathbf{Y}) &\propto \log f(\mathbf{Y}|\theta) + \log \pi(\theta)\\
&= \left[\sum_{i=1}^n \log f(Y_i|\theta) \right] + \log \pi(\theta)
\end{align*}
$$
What a prior sampling statement such as `theta ~ normal(0, 1);` actually does is *increment* the log posterior. Similarly, a "likelihood" sampling statement such as `y ~ normal(theta, 1)` actually does is loop over all elements of the vector `y` to increment the log posterior by 
$$
\log f(Y_i|\theta) = (\text{Constant}) - \frac{1}{2} (y_i - \theta)^2.
$$
Sampling statements drop the constant term. Another way of building up the log posterior that keeps the constants but is more explicit about what's actually happening is the following:
```
for(i in 1:10) {
  target += normal_lpdf(y[i] | mu, 1) 
}
```
So that's what `target` is! So why does it appear in the STAN code generated by `ulam()` above? The answer can be found in the chapter on [finite mixtures](https://mc-stan.org/docs/stan-users-guide/mixture-modeling.html) from the STAN user guide. The key point here is that STAN cannot sample discrete parameters, so we need to "integrate them out" instead. There's an example of the zero-inflated Poisson model in [this section](https://mc-stan.org/docs/stan-users-guide/zero-inflated.html). I'll summarize the main ideas here.

- We program the log likelihood "by hand" using the `target +=` approach described above. 
- `log_mix()` can be used in place of `log_sum_exp()` to [vectorize a mixture](https://mc-stan.org/docs/stan-users-guide/vectorizing-mixtures.html)
- For numerical stability, we rely on several helper functions: `poisson_lpfm()` (but probably we want the log-parameterized version?), `log1m()` and `log_sum_exp()`.
- Maybe start with a flat prior version (just omit the priors) to keep things simple. Then there's no need to deal with the log-parameterization.
- It's possible to implement this [more efficiently](https://mc-stan.org/docs/stan-users-guide/zero-inflated.html#optimizing-the-zero-inflated-poisson-model) by doing some initial calculations for the zero counts.












