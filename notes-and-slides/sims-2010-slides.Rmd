---
title: "'Understanding Non-Bayesians'"
author: "Francis J. DiTraglia"
date: "Summer of Bayes 2024"
institute: "University of Oxford"
output: beamer_presentation
classoption: aspectratio=169
header-includes:
- \setbeamertemplate{navigation symbols}{}
- \setbeamertemplate{footline}[page number]
- \usepackage{tikz}
- \usepackage{multirow}
- \usepackage{booktabs} % needed for modelsummary
- \usepackage{siunitx} % needed for modelsummary
- \usepackage{bbm}
- \usetikzlibrary{positioning,arrows,fit,decorations.pathreplacing,shapes.geometric,calc}
- \renewcommand{\tightlist}{\setlength{\itemsep}{1.2ex}\setlength{\parskip}{0pt}}
- \newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
urlcolor: blue
---

```{r global_options, include=FALSE}
rm(list=ls()) ### To clear namespace
library(knitr)
opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

---

## Goals of this Presentation^[Unless otherwise indicated, all quotes are from Sims (2010).]

1. Summarize some key points from [Sims (2010) - Understanding Non-Bayesians](http://sims.princeton.edu/yftp/UndrstndgNnBsns/GewekeBookChpter.pdf).
2. Relate them to the broader discussion of Bayesian vs.\ Frequentist inference.


---

## Fun Facts 

:::: {.columns}
::: {.column width="70%"}

\vspace{2em}

- "Understanding Non-Bayesians" was originally written for the [Handbook of Bayesian Econometrics](https://doi.org/10.1093/oxfordhb/9780199559084.001.0001) in 2010.
- Oxford University Press objected to Sims posting a pre-print on [his website](https://www.princeton.edu/~sims/).
- Sims favors open access and withdrew from the handbook in protest; the paper remains unpublished. 
- In 2011 [Sims](https://en.wikipedia.org/wiki/Christopher_A._Sims) was awarded the [Economics Nobel](https://www.nobelprize.org/prizes/economic-sciences/2011/summary/) for "understanding cause and effect in the macroeconomy."
- Take that OUP!

:::
::: {.column width="30%"}
![Chris Sims in 2011](Sims-in-2011.jpg)
:::

::::

---

## Motivation: Why isn't everyone Bayesian?

:::: {.columns}
::: {.column width="70%"}
> Once one becomes used to thinking about inference from a Bayesian perspective, it becomes difficult to understand why many econometricians are uncomfortable with that way of thinking. But some very good econometricians are either firmly non-Bayesian or (more commonly these days) think of Bayesian approaches as a "tool" which might sometimes be appropriate, sometimes not.

\vspace{1em}

### Some Pedantry

Maybe we should call it "Bayes-Laplace" inference: [Laplace](https://en.wikipedia.org/wiki/Pierre-Simon_Laplace#Analytic_theory_of_probabilities) developed what we would recognize as "Bayesian" inference.

:::

:::{.column width="30%"}

![Laplace in 1775](Laplace-in-1775.jpg)



:::

::::

---

## What is this paper about?


- Purpose: Articulate counterarguments to Bayesian perspective
- Some counterarguments are "easily dismissed"
- Others relate to deep questions about inference in infinite-dimensional spaces

\vspace{2em}

> My conclusion is that the Bayesian perspective is indeed universally applicable, but that "non-parametric" inference is hard, in ways about which both Bayesians and non-Bayesians are sometimes careless.


---

## Differing Interpretations of Probability

- Crucial background to the differences between Bayesians and Frequentists.
- Math is the same either way (Kolmogorov Axioms) but meaning is different.
- Bayesians: **"Belief-type"**  interpretation. 
- Frequentists: **"Frequency-type"**  interpretation.
- Sims doesn't discuss this. The next two slides (including quotes) are based on Chapter 11 of [An Introduction to Probability and Inductive Logic](https://www.cambridge.org/highereducation/books/an-introduction-to-probability-and-inductive-logic/EA38518337057366AD690C789DD546B8?utm_campaign=shareaholic&utm_medium=copy_link&utm_source=bookmark) by Ian Hacking.


---

## Belief-Type: "It is probable that the dinosaurs were made extinct by a giant asteroid hitting the earth."

### Interpersonal / Evidential: Keynes, Jeffreys, Jaynes 
- "Relative to the available evidence, the probability that the dinosaurs were made extinct by a giant asteroid hitting the earth is high--about 0.9."
- "She thinks that [the statement] is *interpersonal*--because it is about what is reasonable for any reasonable person to believe. And since the degree of belief should depend on the available *evidence* we call this *interpersonal/evidential*."

### Personal Degree of Belief: de Finetti, Savage
- "I personally am very confident that the dinosaurs were made extinct by a giant asteroid hitting earth."
- "If I had to make a bet on it, I would bet 9 to 1 that the dinosaurs were made extinct by a giant asteroid hitting the earth."


---

## Frequency-Type: "The probability of getting heads with this coin is 0.6." 

> The truth of this statement seems to have nothing to do with what be believe. We seem to be making a completely factual statement about a material object, namely the coin ... We may be saying something like:
>
>   - In repeated tossing, the *relative frequency* of heads settles down to a stable proportion, 6/10.
>   - The coin has a *tendency* to come down heads far more often than tails.
>   - It has a *propensity* or *disposition* to favor heads.
>   - Or we are saying something more basic about the asymmetry of the coin and the tossing device. We may be referring to the *geometry* and *physics* of the coin, which cause it to come down more often heads than tails.


---

## Learn some \$*@%\&!\# physics before you talk to me about coin flips!^[See Chapter of 10 of Jaynes' [Probability Theory: The Logic of Science](https://bayes.wustl.edu/etj/prob/book.pdf) for more discussion.]




---

## Bayesian versus Frequentist Approaches

### Frequentist 
- "Insists on a sharp distinction between unobserved, but non-random 'parameters' and observable, random data."
- "Works entirely with the probability distributions of data, conditional on unknown parameters--estimators and test statistics, for example--and makes assertions about the distribution of those function of the data, conditional on parameters"

### Bayesian 
- "Treats everything as random before it is observed, and everything observed as, once observed, no longer random." 
- "Aims at assisting in constructing probability statements about anything as yet unobserved (including 'parameters') conditional on the observed data."

---

## Let's unpack this a bit: Frequentists 

- Condition on parameters; make probability statements that average over different *datasets* you could potentially observe.
- E.g. $X_1 ..., X_{100}$ is a random sample from a $\text{N}(\mu, \sigma^2)$ population with $\sigma=1$ known.
- "$\bar{X}_n \pm 1.96 \times \sigma/\sqrt{n}$ is a 95\% confidence interval for $\mu$." 
- Translation: In 95\% of the datasets we could possibly observe, the sample mean will land within about $\pm 0.2$ of the true (fixed and unknown) value of $\mu$.
- The observed interval $\bar{x} \pm 0.2$ either contains $\mu$ or doesn't: nothing is random after we have observed the data. 
- Traditional (Neyman-Pearson) inference is *pre-experimental*: inductive behavior rather than inductive inference.

---

## Let's unpack this a bit: Bayesians

- Condition on *observed data*; make probability statements that average over different *parameter values* that could potentially have generated the data.
- E.g. $X_1 ..., X_{100}$ is a random sample from a $\text{N}(\mu, \sigma^2)$ population with $\sigma=1$ known.
- Need a prior: just for simplicity choose a "vague" one e.g. $\mu \sim \text{N}(0, 10000)$
- "The posterior distribution of $\mu$ is (approximately) $\text{N}(\bar{x}, 1/100)$, so the 95\% highest posterior density interval (HPDI) for $\mu$ is approximately $\bar{x} \pm 0.2$." 
- Translation: Given the observed data, there is around a 95\% probability that the population mean $\mu$ lies within $\pm 0.2$ of the sample mean $\bar{x}$.
- The observed sample mean $\bar{x}$ is fixed and known; the population mean $\mu$ is unknown and treated as random.
- Bayesian inference is *post-experimental* (conditional): inductive inference under an assumed model given observed information.

---

## Why does Sims put the word "parameters" in quotes?

Poirier (1996) textbook on econometrics: chapter 5? 

- "Bayesians take models seriously not literally; Frequentists take models literally not seriously."

- Quote from Porier about specification testing.

---

## Implications for Decision-Making

* Bayesian inference feeds naturally into decision-making under uncertainty
* Frequentist analysis does not directly provide probabilities for decision-makers

---

## "Easily Dismissed" Objection \#1: "Bayesian Inference is Subjective" 

> Researchers who take a Bayesian perspective can take a completely "objective" approach, by aiming at description of the likelihood. Frequentists have no formal interpretation of the global likelihood shape. Frequentist textbook descriptions of methods make no reference to subjective prior beliefs, but everyone recognizes that good applied statistical practice, even for frequentists, entails informal use of prior beliefs when an actual decision is involved. Its supposed "subjectivity" is therefore no reason to forswear the Bayesian approach to inference.

---

## More on "Subjectivity" -- Bayesians 

> Bayesian inference makes the role of subjective prior beliefs in decision-making explicit, and describes clearly how such beliefs should be modified in the light of observations. But most scientific work with data least to publication, not directly to decision-making. That is, most data analysis is aimed at an audience who face different decision problems and may have diverse prior beliefs. In this situation ... useful data analysis summarizes the shape of the likelihood. Sometimes it is helpful to apply non-flat, simple, standardized prior in reporting likelihood shape, but **these are chosen not to reflect the investigator's personal beliefs, but to make the likelihood description more useful to a diverse audience**. A Bayesian perspective makes the entire shape of the likelihood in any sample directly interpretable, whereas a frequentist perspective has to focus on the large-sample behavior of the likelihood near its peak.


---

## More on "Subjectivity" -- Frequentists 

> Though frequentist data analysis makes no explicit use of prior information, **good applied work does use prior beliefs informally even if it is not explicitly Bayesian**. Models are experimented with, and versions that allow reasonable interpretations of the estimated parameter values are favored. Lag lengths in dynamic models are experimented with, and shorter lag lengths are favored if longer ones add little explanatory power. These are reasonable ways to behave, but they are not "objective".


---

## Some Other Views on "Subjectivity"

### Gelman et al (2014) - *Bayesian Data Analysis*
\vspace{0.5em}
> Bayesian methods are sometimes said to be especially subjective because of their reliance on a prior distribution, but in most problems, scientific judgement is necessary to specify both the "likelihood" and "prior" parts of the model. For example, linear regression models are generally at least as suspect as any prior distribution that might be assumed about the regression parameters.

### McElreath (2020) - *Statistical Rethinking*
\vspace{0.5em}

> Within Bayesian data analysis in the natural and social sciences, the prior is considered to be just part of the model. As such it should be chosen, evaluated, and revised just like all the other components of the model ... priors and Bayesian data analysis are no more inherently subjective thatn likelihoods and the repeated sampling assumptions required for significance testing. 

<!--
### Johnson, Ott & Dogucu (2022) - Bayes Rules! Intro to Applied Bayesian Modeling
\vspace{0.5em}

> Frequentist methods were embraced as the superior *objective* alternative to the *subjective* Bayesian philosophy. This subjective stigma is slowly fading for several reasons. First, the "subjective" label can be stamped on *all* statistical analyses, whether frequentist or Bayesian. Our prior knowledge naturally informs what we measure, why we measure it, and how we model it. Second, post-Enlightenment, "subjective" is no longer such a dirty word.
-->

---

## Fighting Words... 


### Jaynes (2003) - *Probability Theory: The Logic of Science*
\vspace{0.5em}

> Today one wonders how it is possible that orthodox logic continues to be taught in some places year after year and praised as "objective", while Bayesians are charged with "subjectivity". Orthodoxians, preoccupied with fantasies about nonexistent data sets and, in principle, unobservable limiting frequencies – while ignoring relevant prior information – are in no position to charge anybody with "subjectivity". If there is no sufficient statistic, the orthodox accuracy claim based on a single "statistic" simply ignores not only the prior information, but also all the evidence in the data that is relevant to that accuracy: hardly an "objective" procedure. If there are ancillary statistics and the orthodoxian follows Fisher by conditioning on them, he obtains just the estimate that Bayes' theorem based on a noninformative prior would have given him by a shorter calculation. Bayes' theorem would have given also a defensible accuracy claim.


---

## Easily Dismissed Objection \#2: "Bayesian Inference is Harder"

- Frequentist inference is often (wrongly) conflated with "convenient and intuitively appealing estimators" asymptotic approximations. 
- Could instead aim for exact finite-sample distribution theory and fully efficient estimators but this is often intractable.
- "Easier to characterize optimal small-sample inference from a Bayesian perspective, and much of the Bayesian literature has insisted that this is a special advantage."
- Simulation-based methods make it easy to explore the shape of complicated likelihood functions in large, non-linear models.

---

## From Chapter 1 of *Asymptotic Statistics* by van der Vaart 

> For a relatively small number of statistical problems there exists and exact, optimal solution ... If exact optimality theory does not give results, be it because the problem is intractable or because there exist no "optimal" procedures, then asymptotic optimality theory may help ... **strictly speaking, most asymptotic results that are currently available are logically useless. This is because most asymptotic results are limit results, rather than approximations consisting of an approximating formula plus an accurate error bound** ... This is why there is good asymptotics and bad asymptotics and why two types of asymptotics sometimes lead to conflicting claims ... Because it may be theoretically very hard to ascertain that approximation errors are small, one often takes recourse to simulation studies 

---

## But what about GMM, IV and Kernel Smoothing?!

- "There is no reason in principle that very easily implemented estimators like [these] ... have to be interpreted from a Frequentist perspective."
- Kwan (1998) if $\sqrt{n} (\hat{\beta} - \beta) \to_d \text{Normal}(0, \Sigma)$ then, under regularly conditions and for large $n$, we can approximate $\beta|\widehat{\beta} \approx \text{Normal}(\widehat{\beta}, \Sigma/n)$.
- When the regularity conditions hold, can interpret Frequentist confidence intervals as (approximate) Bayesian posterior credible intervals.
- Only conditions on $\widehat{\beta}$, not the full data: "throws information away."

---

## Stronger Result

> If we have a trustworthy model for which the likelihood function can be computed, the likelihood function will, under regularity conditions, take on a Gaussian shape in large samples, with the mean at the maximum likelihood estimate (MLE) and the covariance matrix given by the usual frequentist estimator for the covariance matrix of an MLE.

### Differences from Kwan (1998) Result

1. For large $n$, conditioning on MLE and using a large-sample normal approximation doesn't throw away information: as good as conditioning on all the data.
2. Only a shortcut: from a Bayesian perspective, can always check the quality of the approximation by examining the whole likelihood function.
3. Lacks "robustness" in that: "interpreting the likelihood shape as the posterior generally requires believing that the likelihood is correctly specified." 


---

## Modern Simulation-based Methods 

- Probabilistic programming languages (PPLs) like STAN, JAGS, BUGS, PyMC3 make it extremely easy to build and estimate complex Bayesian models.
- We'll have a session on this in September.
- [StanCon 2024](https://mc-stan.org/events/stancon2024/) is in Oxford this year! (September 9-13th, 2024) 

---


## Less Easily Dismissed Objections

### Handy methods that seem un-Bayesian
* IV, GMM, sandwich estimators, kernel methods
* Can be given limited information Bayesian interpretations
* Involve implicit Bayesian judgments in asymptotic theory

---

## Challenges in Non-parametrics

* Infinite-dimensional parameter spaces
* Consistency issues in Bayesian inference
* Pitfalls in high-dimensional spaces:
  - Priors can be unintentionally dogmatic
  - Importance of careful prior specification

---

## Example: Angrist and Krueger (1991) Quarter of Birth

- The Wasserman problem is about non-parametrics and you can read about it here.
- But we don't need anything too exotic to see the issues Sims is talking about.
- If you're not an economist and don't know what instrumental variables is, here's a very quick introduction.
- Give the introduction.
- Then make the point of Chamberlain \& Imbens (1996)
- Point out that the Frequentist solution is also terrible in this case since it corresponds to an insane prior!
- Useful dialogue between Bayesians and Frequentists: what prior does the frequentist solution correspond to? Frequency properties of Bayesian estimators?

---

## Example 1: The Wasserman Problem

* Setup: Observing $(\xi, R, Y)$ with unobserved $\theta$ 
* Goal: Estimate $\psi = \mathbb{E}[\theta]$
* Bayesian approaches:
  1. Independence case
  2. Dependence case (sieve method)
  3. Limited information approach

---

## Critique of Wasserman's Conclusions

I probably still want to mention these points, but I don't really want to get into the Wasserman example since it won't be as familiar to the audience.

* Bayesian methods are not necessarily insensitive to data
* Importance of appropriate prior specification
* Pitfalls of high-dimensional parameter spaces

---

## Example 2: Robust Variance Estimates in Regression

Not sure how much I should say about this one, but if I do mention it then it might be worth mentioning the Leamer "White-washing" stuff along with the paper where he talks about the "sandwich" estimator versus GLS and something about when the point estimates will change.

* OLS with sandwich covariance matrix
* Efficiency bounds (Chamberlain, 1987)
* When is OLS with sandwich appropriate?
  - Large samples
  - Likely nonlinear regression function
  - Interest in best linear predictor

---

## Conclusion

* Bayesian perspective is universally applicable
* Importance of careful modeling in high-dimensional spaces
* Pragmatic Bayesian approach:
  - Recognize limitations of asymptotic approximations
  - Consider model improvements when appropriate
  - Use OLS with sandwich judiciously

---

## Questions?

